{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient methods cont.\n",
    "### REINFORCE with baseline and actor-critic methods\n",
    "RLDMUU, UniNE 2025, jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start with setting up PyTorch. As usual, we get the device on which we will compute everything, and also we will set the random seed to make the results reproducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running mean function for the purpose of visualization\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE WITH BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we've seen how to implement REINFORCE algorithm, and how standardizing the returns $G$ made learning more stable. Another way to stabilize the learning process by reducing variance is to use some baseline algorithm $b(s)$, which would give us the expected return at state $s$. We can then use another parametrized, differentiable function to approximate the value at a given state. We can reuse the policy $\\pi(a|s, \\bf{\\theta})$ from the previous exercise, while adding a state value function $\\hat{v}(s, \\bf{w})$, with its own parameters and separate learning rates $\\alpha_\\theta$ and $\\alpha_{\\bf{w}}$. Then, just as it was the case with previous exercise, for each episode we collect its trajectory $\\tau$ and compute the following:\n",
    "\n",
    "- generate a trajectory $\\tau$ following policy $\\pi(\\cdot | \\cdot, \\theta)$\n",
    "- for each $t$ in $\\tau$:\n",
    "    - $G_t \\leftarrow \\sum_{k=t+1}^{T} \\gamma^{k-t-1} r_k$\n",
    "\n",
    "Now however, instead of updating the network with discounted rewards in the future, we calculate the advantage term $\\delta$ for each timestep $t$:\n",
    "\n",
    "- $\\delta \\leftarrow G - \\hat{v}(s_t, \\bf{w})$\n",
    "\n",
    "Then value network might be optimized with respect to the advantage term, while the update of the the policy network is conducted as follows:\n",
    "\n",
    "- $\\theta \\leftarrow \\theta + \\alpha_\\theta \\gamma^t \\delta \\nabla ln \\pi(a_t | s_t, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, hidden_dim_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # 2 fully connected layers\n",
    "        self.linear1 = nn.Linear(n_inputs, hidden_dim_size)\n",
    "        self.linear2 = nn.Linear(hidden_dim_size, n_outputs)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        # instead of returning one output, let's return logπ together with π\n",
    "        probs = F.softmax(self.linear2(x), dim=-1)\n",
    "        log_probs = torch.log(probs)\n",
    "\n",
    "        return probs, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the value network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_states, hidden_dim):\n",
    "        pass\n",
    "\n",
    "    def forward(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize the value network and its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still going to use the `CartPole` environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE with baseline main loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:36<00:00, 12.76it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAJECTORIES = 2000\n",
    "MAX_EPISODE_LENGTH = 500\n",
    "gamma = 0.9\n",
    "# placeholders for rewards for each episode\n",
    "rewards = []\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "# iterating through trajectories\n",
    "for tau in tqdm(range(NUM_TRAJECTORIES)):\n",
    "    # resetting the environment\n",
    "    state, info = env.reset()\n",
    "    # setting done to False for while loop \n",
    "    done = False\n",
    "    # storing trajectory and logπ(a_t|s_t, θ)\n",
    "    transition_buffer = []\n",
    "    log_probs = []\n",
    "    state_values =[]\n",
    "    \n",
    "    t = 0\n",
    "    while done == False and t < MAX_EPISODE_LENGTH:\n",
    "        # TODO: play the episode and  collect the data\n",
    "        pass\n",
    "    # logging the episode length as a cumulative reward\n",
    "    rewards.append(t)\n",
    "    returns = []\n",
    "    for t_prime in range(t):\n",
    "        # computing discounted rewards in future for every timestep\n",
    "        G = 0\n",
    "        for i, tick in enumerate(transition_buffer[t_prime:]):\n",
    "            G += (gamma ** i) * tick\n",
    "        returns.append(G)\n",
    "\n",
    "    # turning the returns vector into a tensor\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    # TODO: compute the advantage term δ\n",
    "    deltas = ...\n",
    "    \n",
    "    # TODO: perform update for both policy and value network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(running_mean(rewards, 50))\n",
    "plt.grid()\n",
    "plt.title(\"REINFORCE with baseline cumulative rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-step actor-critic\n",
    "\n",
    "Another approach is to update the policy not at the end of each trajectory, but at each timestep using the 1 step return. Therefore when computing $\\delta$ advantage term, we can use not the discounted rewards from all timesteps until the terminal state, but rather:\n",
    "\n",
    "- $G_{t:t+1} \\leftarrow r + \\gamma \\hat{v}(s', \\bf{w})$\n",
    "- $\\delta \\leftarrow G_{t:t+1} - \\hat{v}(s, \\bf{w})$\n",
    "\n",
    "More specifically, we can use the existing policy to compute the advantage term in the online fashion:\n",
    "\n",
    "- $\\delta \\leftarrow r + \\gamma \\hat{v}_{\\pi}(s', \\textbf{w}) - \\hat{v}(s, \\bf{w})$\n",
    "\n",
    "Then we can update the value network with:\n",
    "\n",
    "$\\theta_v \\leftarrow \\theta_v + \\alpha_\\theta \\delta^2 \\nabla \\hat{v}$\n",
    "\n",
    "and the update for policy network:\n",
    "\n",
    "$\\theta_\\pi \\leftarrow \\theta_\\pi + \\alpha_\\theta \\gamma^t \\delta \\nabla ln \\pi(a_t | s_t, \\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reuse the policy and value networks from the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNetwork(n_inputs=4, n_outputs=2, hidden_dim_size=128).to(device)\n",
    "value = ValueNetwork(num_states=4, hidden_dim=128).to(device)\n",
    "policy_optimizer = torch.optim.Adam(params=policy.parameters(), lr=1e-4)\n",
    "value_optimizer = torch.optim.Adam(params=value.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAJECTORIES = 1000\n",
    "MAX_EPISODE_LENGTH = 500\n",
    "gamma = 0.99\n",
    "# placeholders for rewards for each episode\n",
    "rewards = []\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "# iterating through trajectories\n",
    "for tau in tqdm(range(NUM_TRAJECTORIES)):\n",
    "    # resetting the environment\n",
    "    state, info = env.reset(seed=123)\n",
    "    # setting done to False for while loop \n",
    "    done = False\n",
    "\n",
    "    t = 0\n",
    "    while done == False and t < MAX_EPISODE_LENGTH:\n",
    "        # TODO: perform the actor-critic update\n",
    "        t += 1\n",
    "    rewards.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(running_mean(rewards, 50))\n",
    "plt.grid()\n",
    "plt.title(\"1-step actor-critic cumulative rewards\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
