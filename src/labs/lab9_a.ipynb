{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient\n",
    "### REINFORCE ALGORITHM - MONTE CARLO APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define our policy network $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, hidden_dim_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # TODO: Declare the structure of the network\n",
    "\n",
    "    def forward(self, state):\n",
    "        # TODO: return logπ together with π, using torch.log() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the CartPole environment\n",
    "# https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final loop:\n",
    "\n",
    "- generate a trajectory $\\tau$ following policy $\\pi(\\cdot | \\cdot, \\theta)$\n",
    "- for each $t$ in $\\tau$:\n",
    "    - $G_t \\leftarrow \\sum_{k=t+1}^{T} \\gamma^{k-t-1} r_k$\n",
    "    - $\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla \\ln \\pi (a_t|s_t, \\theta)$\n",
    "\n",
    "with $G_t$ being the discounted reward in future at timestep $t$, $\\theta$ meaning policy network parameters and trajectory $tau$ being the set of states, actions and rewards $(s_0, a_0, r_0, s_1, ..., s_T, a_T, r_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAJECTORIES = 2000\n",
    "MAX_EPISODE_LENGTH = 500\n",
    "gamma = 0.9\n",
    "# placeholders for rewards for each episode\n",
    "rewards = []\n",
    "losses = []\n",
    "# iterating through trajectories\n",
    "for tau in tqdm(range(NUM_TRAJECTORIES)):\n",
    "    # resetting the environment\n",
    "    state, info = env.reset()\n",
    "    # setting done to False for while loop \n",
    "    done = False\n",
    "    # storing trajectory and logπ(a_t|s_t, θ)\n",
    "    transition_buffer = []\n",
    "    log_probs = []\n",
    "    \n",
    "    t = 0\n",
    "    while done == False and t < MAX_EPISODE_LENGTH:\n",
    "        # TODO: Play the episode and collect data\n",
    "        pass\n",
    "    # logging the episode length as a cumulative reward\n",
    "    rewards.append(t)\n",
    "    returns = []\n",
    "    for t_prime in range(t):\n",
    "        # TODO: Compute G\n",
    "        pass\n",
    "    # TODO: Normalize the returns and perform and update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results of the training\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(running_mean(rewards,100))\n",
    "plt.grid()\n",
    "plt.title(\"REINFORCE cumulative rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rewards, columns=['reward'])\n",
    "df.to_csv('REINFORCE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not normalizing returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the model and the optimizer again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Repeat training without normalizing G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(running_mean(rewards,100), label=\"normalized\")\n",
    "plt.plot(running_mean(rewards_non_norm, 100), label=\"not normalized\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(\"REINFORCE cumulative rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Chapter 13.3 of Sutton&Barto \"Reinforcement Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information about normalizing the discounted rewards:\n",
    " - http://karpathy.github.io/2016/05/31/rl/, paragraph *More general advanced functions*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
