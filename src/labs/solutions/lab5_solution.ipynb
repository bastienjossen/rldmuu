{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### Policy Iteration\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMDP:\n",
    "    def __init__(self, n_states, n_actions, P = None, R = None):\n",
    "        self.n_states = n_states \n",
    "        self.n_actions = n_actions \n",
    "        if (P is None):\n",
    "            self.P = np.zeros([n_states, n_actions, n_states]) \n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.P[s,a] = np.random.dirichlet(np.ones(n_states))\n",
    "        else:\n",
    "            self.P = P\n",
    "        if (R is None):\n",
    "            self.R = np.zeros([n_states, n_actions])\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.R[s,a] = np.round(np.random.uniform(), decimals=1)\n",
    "        else:\n",
    "            self.R = R\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                assert(abs(np.sum(self.P[s,a,:])-1) <= 1e-3)\n",
    "                assert((self.P[s,a,:] <= 1).all())\n",
    "                assert((self.P[s,a,:] >= 0).all())\n",
    "                \n",
    "    def get_transition_probability(self, state, action, next_state):\n",
    "        return self.P[state, action, next_state]\n",
    "    \n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        return self.P[state, action]\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        return self.R[state, action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChainMDP(DiscreteMDP):\n",
    "    \"\"\"\n",
    "    Problem where we need to take the same action n_states-1 time in a row to get a highly rewarding state\n",
    "    The optimal policy greatly depends on the discount factor we choose.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_states=20):\n",
    "        assert  n_states > 1\n",
    "\n",
    "        n_actions = 2\n",
    "        super().__init__(n_states=n_states, n_actions=n_actions)\n",
    "\n",
    "        self.R[:] = 0.\n",
    "        self.P[:] = 0.\n",
    "\n",
    "        self.R[:, 1] = -1 / (n_states-1)\n",
    "        self.R[n_states-1, 1] = 1.\n",
    "        self.R[:, 0] = 1/n_states\n",
    "\n",
    "        for i in range(self.n_states-1):\n",
    "            if i > 0:\n",
    "                self.P[i, 0, i-1] = 1.\n",
    "            else:\n",
    "                self.P[i, 0, i] = 1.\n",
    "\n",
    "            self.P[i, 1, i+1] = 1.\n",
    "\n",
    "        self.P[self.n_states-1, :, self.n_states-1] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task will be to implement the policy iteration algorithm. Let's start with policy evaluation. Given a policy $\\pi$, you have to evaluate this policy on all states. \n",
    "\n",
    "$$ V^{\\pi}(s) = \\sum_{s'} P(s' | s, \\pi(s)) [R(s, \\pi(s)) + \\gamma V^{\\pi}(s')] $$ \n",
    "\n",
    "You can either program the policy evaluation using dynamic programming, or by using the equation:\n",
    "\n",
    "$$ V^{\\pi} = \\left[\\mathbb{I} - \\gamma \\mathbb{P} \\right]^{-1} r $$ \n",
    "\n",
    "where $\\mathbb{I}$ is an identity matrix, $\\mathbb{P}$ a probability matrix and $r$ a reward vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp = ChainMDP()\n",
    "np.array([mdp.get_transition_probabilities(s, 0) for s in range(mdp.n_states)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_dynamic_programming(mdp: DiscreteMDP, gamma: float, policy: list[int], n_iters: int):\n",
    "    V = np.zeros(mdp.n_states)\n",
    "    Q = np.zeros(mdp.n_states)\n",
    "    for _ in range(n_iters):\n",
    "        for s in range(mdp.n_states):\n",
    "            a = int(policy[s])\n",
    "            P = mdp.get_transition_probabilities(s, a)\n",
    "            Q[s] = mdp.get_reward(s, a)\n",
    "            for next_s in range(mdp.n_states):\n",
    "                Q[s] += gamma * P[next_s] * V[next_s]\n",
    "        V = Q\n",
    "    return V\n",
    "\n",
    "def policy_evaluation_matrix_multiplication(mdp: DiscreteMDP, gamma: float, policy: list[int]):\n",
    "    I = np.eye(mdp.n_states)\n",
    "    R = np.array([mdp.R[s, int(policy[s])] for s in range(mdp.n_states)])\n",
    "    P = np.array([mdp.get_transition_probabilities(s, int(policy[s])) for s in range(mdp.n_states)])\n",
    "    return np.linalg.inv(I - gamma * P) @ R  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement policy iteration by evaluating policy at each state and set the policy to:\n",
    "\n",
    "$$ \\pi(s) = \\arg\\max_{a \\in A} Q^\\pi (s,a) $$\n",
    "\n",
    "Iterate until you reach maximal number of iterations or until newly computed values don't differ by more than some $\\epsilon$ or until $\\pi$ doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int, n_eval_iters: int, use_dp: bool = False):\n",
    "    policy = np.zeros(mdp.n_states)\n",
    "    V = np.zeros(mdp.n_states)\n",
    "    Q = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "    for _ in range(n_iters):\n",
    "        if use_dp:\n",
    "            V = policy_evaluation_dynamic_programming(mdp, gamma, policy, n_eval_iters)\n",
    "        else:\n",
    "            V = policy_evaluation_matrix_multiplication(mdp, gamma, policy)\n",
    "\n",
    "        # Policy improvement\n",
    "        for s in range(mdp.n_states):\n",
    "            for a in range(mdp.n_actions):\n",
    "                P = mdp.get_transition_probabilities(s,a)\n",
    "                Q[s,a] = mdp.get_reward(s, a) \n",
    "                for next_s in range(mdp.n_states):\n",
    "                    Q[s,a] += gamma * P[next_s] * V[next_s]\n",
    "            policy[s] = np.argmax(Q[s,:])\n",
    "    \n",
    "    return policy, V\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second task will be to implement Value Iteration algorithm. At each timestep $t$, you have to compute a new value function by maximizing the Bellman equation.\n",
    "\n",
    "$$ V_t (s) = \\max_a \\left( \\sum_{s'} P(s'|s, a) [R(s,a) + \\gamma V_{t-1}(s')] \\right) $$\n",
    "\n",
    "Once $V$ converges to some $V^{*}$ (or once you reach the limit of iterations), you can extract the policy for each state:\n",
    "\n",
    "$$ \\pi^{*}(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) [R(s,a) + \\gamma V^{*}(s')] $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int):\n",
    "    policy = np.zeros(mdp.n_states)\n",
    "    V = np.zeros(mdp.n_states)\n",
    "    Q = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "    for _ in range(n_iters):\n",
    "        for s in range(mdp.n_states):\n",
    "            for a in range(mdp.n_actions):\n",
    "                Q[s,a] = mdp.get_reward(s,a)\n",
    "                P = mdp.get_transition_probabilities(s,a)\n",
    "                for next_s in range(mdp.n_states):\n",
    "                    Q[s,a] += gamma * P[next_s] * V[next_s]\n",
    "        V = np.max(Q, axis=1)\n",
    "        policy = np.argmax(Q, axis=1)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Run both methods on the provided MDP for a given Chain MDP instance and compare the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLICY ITERATION\n",
      "POLICY:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "V\n",
      "[ 0.89563339  1.05362774  1.22917702  1.42423178  1.64095929  1.88176763\n",
      "  2.14933245  2.4466267   2.77695364  3.14398358  3.55179462  4.004918\n",
      "  4.50838842  5.0678      5.68936842  6.38        7.14736842  8.\n",
      "  8.94736842 10.        ]\n",
      "VALUE ITERATION\n",
      "POLICY:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "V\n",
      "[ 0.89563339  1.05362774  1.22917702  1.42423178  1.64095929  1.88176763\n",
      "  2.14933245  2.4466267   2.77695364  3.14398358  3.55179462  4.004918\n",
      "  4.50838842  5.0678      5.68936842  6.38        7.14736842  8.\n",
      "  8.94736842 10.        ]\n"
     ]
    }
   ],
   "source": [
    "mdp = ChainMDP()\n",
    "\n",
    "N_ITERS = 10_000\n",
    "N_EVAL_ITERS = 100\n",
    "GAMMA = .9\n",
    "\n",
    "policy, V = policy_iteration(mdp, GAMMA, N_ITERS, N_EVAL_ITERS)\n",
    "print(\"POLICY ITERATION\")\n",
    "print(f\"POLICY:\\n{policy}\")\n",
    "print(f\"V\\n{V}\")\n",
    "policy, V = value_iteration(mdp, GAMMA, N_ITERS)\n",
    "print(\"VALUE ITERATION\")\n",
    "print(f\"POLICY:\\n{policy}\")\n",
    "print(f\"V\\n{V}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
