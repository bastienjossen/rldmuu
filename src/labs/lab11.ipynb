{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47820f13",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### UCRL\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e53a35",
   "metadata": {},
   "source": [
    "We continue exploring more advanced approaches to reinforcement learning, this time taking a look at [UCRL](https://papers.nips.cc/paper_files/paper/2006/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf). The main idea is, that when estimating rewards and transition probabilities, we can maintain a set of possible MDPs that fit our problem, by calculating the confidence bounds. Then we optimistically assume that the MDP with the biggest reward is the correct one, and we compute the policy, for example by using value iteration.\n",
    "\n",
    "In today's task we can use the original bounds for rewards and transitions as presented in paper, that is respectively:\n",
    "\n",
    "$$ \\text{conf}_r (t,s,a) = \\min \\left\\{ 1, \\sqrt{\\frac{\\log(2 t^{\\alpha} |S| |A|)}{2 N_t (s,a)}} \\right\\} $$\n",
    "\n",
    "$$ \\text{conf}_p (t,s,a) = \\min \\left\\{ 1, \\sqrt{\\frac{\\log(4 t^{\\alpha} |S|^2 |A|)}{2 N_t (s,a)}} \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a25e6",
   "metadata": {},
   "source": [
    "While the estimates for $\\hat{r}_t (s,a)$ and $\\hat{p}_t (s, a, s')$, are just:\n",
    "\n",
    "$$ \\hat{r}_t (s,a) = \\frac{R_t (s,a)}{N_t (s,a)} $$\n",
    "\n",
    "$$ \\hat{p}_t (s, a, s') = \\frac{P_t (s,a,s')}{N_t (s,a)} $$\n",
    "\n",
    "where $R_t (s,a)$, $P_t(s,a,s')$ and $N_t (s,a)$ are the sums of rewards, transitions to $s'$ from $(s,a)$ and number of times visited, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b9025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCRL:\n",
    "    def __init__(self, states, actions, alpha):\n",
    "        self.num_states = states \n",
    "        self.num_actions = actions \n",
    "        self.alpha = alpha \n",
    "        # TODO: initialize R_t, P_t and N_t\n",
    "\n",
    "        # TODO: initialize a policy\n",
    "\n",
    "    def act(self,state):\n",
    "        # TODO: act greedily\n",
    "        pass\n",
    "\n",
    "    def get_confidence_bounds(self):\n",
    "        # TODO: get P and R estimates \n",
    "        # TODO: get confidence bounds \n",
    "        pass\n",
    "\n",
    "    def update_policy(self, r_estimate, r_bound, p_estimate, p_bound):\n",
    "        # TODO: get the most optimistic rewards and transitions within the confidence intervals\n",
    "\n",
    "        # TODO: perform value iteration and update greedy policy\n",
    "        pass\n",
    "\n",
    "    def update_counters(self, state, action, next_state):\n",
    "        # TODO: Update R_t, P_t and N_t\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "N_EPISODES = 10000\n",
    "N_ITER = 1000\n",
    "\n",
    "ALPHA = 0.1\n",
    "\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "\n",
    "algo = UCRL(states=env.observation_space.n, actions=env.action_space.n, alpha=ALPHA)\n",
    "\n",
    "nsteps = np.ones(N_EPISODES) * N_ITER\n",
    "mean_episode_rewards = np.zeros(N_EPISODES)\n",
    "\n",
    "for e in range(N_EPISODES):\n",
    "    algo.update_policy(algo.get_confidence_bounds())\n",
    "\n",
    "    for i in range(N_ITER):\n",
    "        action = algo.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        algo.update_counters(state, action, next_state)\n",
    "\n",
    "        if done or truncated:\n",
    "            state, info = env.reset()\n",
    "            done = False \n",
    "            truncated = False\n",
    "            if reward == 1:\n",
    "                nsteps[e] = i\n",
    "                mean_episode_rewards[e] = 1 / i\n",
    "            break \n",
    "\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
