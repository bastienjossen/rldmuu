{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75db9e6e",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### Bayesian Reinforcement Learning\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a18d96",
   "metadata": {},
   "source": [
    "In today's lab we are going to take a look at the Bayesian flavor of the Q-Learning. Generally speaking, Bayesian RL takes advantages of Bayesian inference, when dealing with information update. Here, the Q values are sampled from the distribution associated with each state-action pair, while the updates are being performed on conjugate priors. In this tutorial we will assume that Q-values are gaussian distributed with unit variance. *Modus operandi* is very similar to lab 3 and Thompson sampling for gaussian bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b545689",
   "metadata": {},
   "source": [
    "Recall, that updating the conjugate prior in gaussian thompson sampling for multi armed bandits is done by following update rules:\n",
    "\n",
    "$$ \\hat{\\mu}_k' = \\mathbb{E}(\\hat{\\mu}_k' | r) = \\frac{\\hat{\\sigma}_k^2 r + \\sigma_k^2 \\hat{\\mu}_k}{\\hat{\\sigma}_k^2 + \\sigma_k^2} $$ \n",
    "$$ \\hat{\\sigma}_k' = \\sqrt{\\text{Var}(\\hat{\\mu}_k | r)} = \\sqrt{\\frac{\\hat{\\sigma}_k^2 \\sigma_k^2}{\\hat{\\sigma}_k^2 + \\sigma_k^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158648af",
   "metadata": {},
   "source": [
    "In this setting however, we need to substitute the reward term for bandits with the temporal difference target, with additional observation noise, sampled from separate distribution, which in our case will also be gaussian:\n",
    "\n",
    "$$ r + \\gamma \\max_a Q(s', a) + \\epsilon$$\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0, \\sigma^2_{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1421e",
   "metadata": {},
   "source": [
    "To sum up, the procedure for single Bayesian Q Learning update is as follows:\n",
    "\n",
    "- sample the values from conjugate prior distributions for different Q-values at the current state\n",
    "- choose the action maximizing the samples\n",
    "- act on the environment, observe the next state and reward, and use it to update the prior parameters, $\\hat{\\mu}'_k$ and $\\hat{\\sigma}'_k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dba1a9",
   "metadata": {},
   "source": [
    "When calculating the $\\max_a Q(s', a)$ term, you can just use the mean values of conjugate priors, thus avoiding sampling from the posterior and potentially destabilizing the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc759730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianQLearning:\n",
    "    def __init__(self, states, actions, mu_zero=0., gamma=.95, noise_std=1.):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.prior_mu = np.full((states, actions), mu_zero)\n",
    "        self.prior_std = np.full((states, actions), 1.)\n",
    "\n",
    "        self.noise_prior_std = noise_std\n",
    "\n",
    "    def act(self, state):\n",
    "        # TODO: Act by performing a Thompson sampling over Q(s, *)\n",
    "        pass\n",
    "\n",
    "    def update_priors(self, state, action, reward, next_state, terminated, truncated):\n",
    "        # TODO : Calculate the target value used for update\n",
    "        # Don't forget about the observation noise!\n",
    "\n",
    "        # TODO: Update prior using Bayesian rule\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a8ae1",
   "metadata": {},
   "source": [
    "Run the experiments with different values of parameters and log the results. Feel free to use the results from Lab 6 as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "N_EPISODES = 1000\n",
    "N_ITER = 1000\n",
    "\n",
    "MU_ZERO = 0.\n",
    "GAMMA = 0.95\n",
    "NOISE_STD = 1.\n",
    "\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "\n",
    "algo = BayesianQLearning(states=env.observation_space.n, actions=env.action_space.n, mu_zero=MU_ZERO, gamma=GAMMA, noise_std=NOISE_STD)\n",
    "\n",
    "nsteps = np.ones(N_EPISODES) * N_ITER\n",
    "for e in range(N_EPISODES):\n",
    "    for i in range(N_ITER):\n",
    "        action = algo.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        algo.update(state, action, reward, next_state)\n",
    "\n",
    "        if done or truncated:\n",
    "            state, info = env.reset()\n",
    "            done = False \n",
    "            truncated = False\n",
    "            if reward == 1:\n",
    "                nsteps[e] = i\n",
    "            break \n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27df69f",
   "metadata": {},
   "source": [
    "Source and interactive visualizations: [Bayesian perspective on Q Learning](https://brandinho.github.io/bayesian-perspective-q-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
