{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### Policy Iteration\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMDP:\n",
    "    def __init__(self, n_states, n_actions, P = None, R = None):\n",
    "        self.n_states = n_states \n",
    "        self.n_actions = n_actions \n",
    "        if (P is None):\n",
    "            self.P = np.zeros([n_states, n_actions, n_states]) \n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.P[s,a] = np.random.dirichlet(np.ones(n_states))\n",
    "        else:\n",
    "            self.P = P\n",
    "        if (R is None):\n",
    "            self.R = np.zeros([n_states, n_actions])\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.R[s,a] = np.round(np.random.uniform(), decimals=1)\n",
    "        else:\n",
    "            self.R = R\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                assert(abs(np.sum(self.P[s,a,:])-1) <= 1e-3)\n",
    "                assert((P[s,a,:] <= 1).all())\n",
    "                assert((P[s,a,:] >= 0).all())\n",
    "                \n",
    "    def get_transition_probability(self, state, action, next_state):\n",
    "        return self.P[state, action, next_state]\n",
    "    \n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        return self.P[state, action]\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        return self.R[state, action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChainMDP(DiscreteMDP):\n",
    "    \"\"\"\n",
    "    Problem where we need to take the same action n_states-1 time in a row to get a highly rewarding state\n",
    "    The optimal policy greatly depends on the discount factor we choose.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_states=20):\n",
    "        assert  n_states > 1\n",
    "\n",
    "        n_actions = 2\n",
    "        super().__init__(n_states=n_states, n_actions=n_actions)\n",
    "\n",
    "        self.R[:] = 0.\n",
    "        self.P[:] = 0.\n",
    "\n",
    "        self.R[:, 1] = -1 / (n_states-1)\n",
    "        self.R[n_states-1, 1] = 1.\n",
    "        self.R[:, 0] = 1/n_states\n",
    "\n",
    "        for i in range(self.n_states-1):\n",
    "            if i > 0:\n",
    "                self.P[i, 0, i-1] = 1.\n",
    "            else:\n",
    "                self.P[i, 0, i] = 1.\n",
    "\n",
    "            self.P[i, 1, i+1] = 1.\n",
    "\n",
    "        self.P[self.n_states-1, :, self.n_states-1] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task will be to implement the policy iteration algorithm. Let's start with policy evaluation. Given a policy $\\pi$, you have to evaluate this policy on all states. \n",
    "\n",
    "$$ V^{\\pi}(s) = \\sum_{s'} P(s' | s, \\pi(s)) [R(s, \\pi(s)) + \\gamma V^{\\pi}(s')] $$ \n",
    "\n",
    "You can either program the policy evaluation using dynamic programming, or by using the equation:\n",
    "\n",
    "$$ V^{\\pi} = \\left[\\mathbb{I} - \\gamma \\mathbb{P} \\right]^{-1} r $$ \n",
    "\n",
    "where $\\mathbb{I}$ is an identity matrix, $\\mathbb{P}$ a probability matrix and $r$ a reward vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_dynamic_programming(mdp: DiscreteMDP, gamma: float, policy: list[int], n_iters: int):\n",
    "    # TODO: Implement policy evaluation using dynamic programming\n",
    "    pass \n",
    "\n",
    "def policy_evaluation_matrix_multiplication(mdp: DiscreteMDP, gamma: float, policy: list[int]):\n",
    "    # TODO: Implement policy evaluation using matrix operations\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement policy iteration by evaluating policy at each state and set the policy to:\n",
    "\n",
    "$$ \\pi(s) = \\arg\\max_{a \\in A} Q^\\pi (s,a) $$\n",
    "\n",
    "Iterate until you reach maximal number of iterations or until newly computed values don't differ by more than some $\\epsilon$ or until $\\pi$ doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int, n_eval_iters: int, use_dp: bool = False):\n",
    "    # TODO: Implement policy iteration\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second task will be to implement Value Iteration algorithm. At each timestep $t$, you have to compute a new value function by maximizing the Bellman equation.\n",
    "\n",
    "$$ V_t (s) = \\max_a \\left( \\sum_{s'} P(s'|s, a) [R(s,a) + \\gamma V_{t-1}(s')] \\right) $$\n",
    "\n",
    "Once $V$ converges to some $V^{*}$ (or once you reach the limit of iterations), you can extract the policy for each state:\n",
    "\n",
    "$$ \\pi^{*}(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) [R(s,a) + \\gamma V^{*}(s')] $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int):\n",
    "    # TODO: Implement value iteration\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Run both methods on the provided MDP for a given Chain MDP instance and compare the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = ChainMDP()\n",
    "\n",
    "N_ITERS = 10_000\n",
    "N_EVAL_ITERS = 100\n",
    "GAMMA = .9\n",
    "\n",
    "policy, V = policy_iteration(mdp, GAMMA, N_ITERS, N_EVAL_ITERS)\n",
    "print(\"POLICY ITERATION\")\n",
    "print(f\"POLICY:\\n{policy}\")\n",
    "print(f\"V\\n{V}\")\n",
    "policy, V = value_iteration(mdp, GAMMA, N_ITERS)\n",
    "print(\"VALUE ITERATION\")\n",
    "print(f\"POLICY:\\n{policy}\")\n",
    "print(f\"V\\n{V}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
