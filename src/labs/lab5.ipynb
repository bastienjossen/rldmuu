{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### Policy Iteration\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMDP:\n",
    "    def __init__(self, n_states, n_actions, P = None, R = None):\n",
    "        self.n_states = n_states \n",
    "        self.n_actions = n_actions \n",
    "        if (P is None):\n",
    "            self.P = np.zeros([n_states, n_actions, n_states]) \n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.P[s,a] = np.random.dirichlet(np.ones(n_states))\n",
    "        else:\n",
    "            self.P = P\n",
    "        if (R is None):\n",
    "            self.R = np.zeros([n_states, n_actions])\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.R[s,a] = np.round(np.random.uniform(), decimals=1)\n",
    "        else:\n",
    "            self.R = R\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                assert(abs(np.sum(self.P[s,a,:])-1) <= 1e-3)\n",
    "                assert((self.P[s,a,:] <= 1).all())\n",
    "                assert((self.P[s,a,:] >= 0).all())\n",
    "                \n",
    "    def get_transition_probability(self, state, action, next_state):\n",
    "        return self.P[state, action, next_state]\n",
    "    \n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        return self.P[state, action]\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        return self.R[state, action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChainMDP(DiscreteMDP):\n",
    "    \"\"\"\n",
    "    Problem where we need to take the same action n_states-1 time in a row to get a highly rewarding state\n",
    "    The optimal policy greatly depends on the discount factor we choose.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_states=20):\n",
    "        assert  n_states > 1\n",
    "\n",
    "        n_actions = 2\n",
    "        super().__init__(n_states=n_states, n_actions=n_actions)\n",
    "\n",
    "        self.R[:] = 0.\n",
    "        self.P[:] = 0.\n",
    "\n",
    "        self.R[:, 1] = -1 / (n_states-1)\n",
    "        self.R[n_states-1, 1] = 1.\n",
    "        self.R[:, 0] = 1/n_states\n",
    "\n",
    "        for i in range(self.n_states-1):\n",
    "            if i > 0:\n",
    "                self.P[i, 0, i-1] = 1.\n",
    "            else:\n",
    "                self.P[i, 0, i] = 1.\n",
    "\n",
    "            self.P[i, 1, i+1] = 1.\n",
    "\n",
    "        self.P[self.n_states-1, :, self.n_states-1] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task will be to implement the policy iteration algorithm. Let's start with policy evaluation. Given a policy $\\pi$, you have to evaluate this policy on all states. \n",
    "\n",
    "$$ V^{\\pi}(s) = \\sum_{s'} P(s' | s, \\pi(s)) [R(s, \\pi(s)) + \\gamma V^{\\pi}(s')] $$ \n",
    "\n",
    "You can either program the policy evaluation using dynamic programming, or by using the equation:\n",
    "\n",
    "$$ V^{\\pi} = \\left[\\mathbb{I} - \\gamma \\mathbb{P} \\right]^{-1} r $$ \n",
    "\n",
    "where $\\mathbb{I}$ is an identity matrix, $\\mathbb{P}$ a probability matrix and $r$ a reward vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_dynamic_programming(mdp: DiscreteMDP, gamma: float, policy: list[int], n_iters: int):\n",
    "    # TODO: Implement policy evaluation using dynamic programming\n",
    "    V = np.zeros(mdp.n_states)\n",
    "    newV = np.zeros(mdp.n_states)\n",
    "    for _ in range(n_iters):\n",
    "        for s in range(mdp.n_states):\n",
    "            a = policy[s]\n",
    "            transition_probailities = mdp.get_transition_probabilities(s,a)\n",
    "            reward = mdp.get_reward(s,a)\n",
    "            summe = np.sum([transition_probailities[next_s] for next_s in range(mdp.n_states)]) # Equals 1\n",
    "            newV[s] = reward* summe + gamma * np.sum([transition_probailities[next_s] * V[next_s] for next_s in range(mdp.n_states)])\n",
    "        V = newV.copy()\n",
    "    return V\n",
    "\n",
    "def policy_evaluation_matrix_multiplication(mdp: DiscreteMDP, gamma: float, policy: list[int]):\n",
    "    # TODO: Implement policy evaluation using matrix operations\n",
    "    n_states = mdp.n_states\n",
    "    P_pi = np.zeros((n_states, n_states))\n",
    "    r_pi = np.zeros(n_states)\n",
    "    for s in range(n_states):\n",
    "        a = policy[s]\n",
    "        P_pi[s, :] = mdp.P[s, a, :]   # Transition probabilities for state s under action a\n",
    "        r_pi[s] = mdp.R[s, a]\n",
    "    I = np.eye(n_states)\n",
    "    V = np.linalg.inv(I - gamma * P_pi).dot(r_pi)\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement policy iteration by evaluating policy at each state and set the policy to:\n",
    "\n",
    "$$ \\pi(s) = \\arg\\max_{a \\in A} Q^\\pi (s,a) $$\n",
    "\n",
    "Iterate until you reach maximal number of iterations or until newly computed values don't differ by more than some $\\epsilon$ or until $\\pi$ doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int, n_eval_iters: int, use_dp: bool = False):\n",
    "    # TODO: Implement policy iteration\n",
    "    epsilon = 1e-3\n",
    "    n_states = mdp.n_states\n",
    "    n_actions = mdp.n_actions\n",
    "    policy = [0] * n_states\n",
    "    for _ in range(n_iters):\n",
    "        if use_dp:\n",
    "            V = policy_evaluation_dynamic_programming(mdp, gamma, policy, n_eval_iters)\n",
    "        else:\n",
    "            V = policy_evaluation_matrix_multiplication(mdp, gamma, policy)\n",
    "        new_policy = [0] * n_states\n",
    "        for s in range(n_states):\n",
    "            Q = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                transition_probailities = mdp.get_transition_probabilities(s,a)\n",
    "                reward = mdp.get_reward(s,a)\n",
    "                Q[a] = reward + gamma * np.sum([transition_probailities[next_s] * V[next_s] for next_s in range(n_states)])\n",
    "            new_policy[s] = np.argmax(Q)\n",
    "        if np.all(np.array(policy) - np.array(new_policy) < epsilon):\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second task will be to implement Value Iteration algorithm. At each timestep $t$, you have to compute a new value function by maximizing the Bellman equation.\n",
    "\n",
    "$$ V_t (s) = \\max_a \\left( \\sum_{s'} P(s'|s, a) [R(s,a) + \\gamma V_{t-1}(s')] \\right) $$\n",
    "\n",
    "Once $V$ converges to some $V^{*}$ (or once you reach the limit of iterations), you can extract the policy for each state:\n",
    "\n",
    "$$ \\pi^{*}(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) [R(s,a) + \\gamma V^{*}(s')] $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int):\n",
    "    # TODO: Implement value iteration\n",
    "    epsilon = 1e-3\n",
    "    n_states = mdp.n_states\n",
    "    n_actions = mdp.n_actions\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(n_iters):\n",
    "        newV = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            Q = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                transition_probailities = mdp.get_transition_probabilities(s,a)\n",
    "                reward = mdp.get_reward(s,a)\n",
    "                Q[a] = reward + gamma * np.sum([transition_probailities[next_s] * V[next_s] for next_s in range(n_states)])\n",
    "            newV[s] = np.max(Q)\n",
    "        if np.all(np.abs(V - newV) < epsilon):\n",
    "            break\n",
    "        V = newV\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    for s in range(n_states):\n",
    "        Q = np.zeros(n_actions)\n",
    "        for a in range(n_actions):\n",
    "            transition_probailities = mdp.get_transition_probabilities(s,a)\n",
    "            reward = mdp.get_reward(s,a)\n",
    "            Q[a] = reward + gamma * np.sum([transition_probailities[next_s] * V[next_s] for next_s in range(n_states)])\n",
    "        policy[s] = np.argmax(Q)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Run both methods on the provided MDP for a given Chain MDP instance and compare the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLICY ITERATION\n",
      "POLICY:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "V\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5]\n",
      "VALUE ITERATION\n",
      "POLICY:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "V\n",
      "[0.88608334 1.04407769 1.21962697 1.41468173 1.63140924 1.87221758\n",
      " 2.1397824  2.43707665 2.76740359 3.13443353 3.54224457 3.99536795\n",
      " 4.49883837 5.05824995 5.67981837 6.37044995 7.13781837 7.99044995\n",
      " 8.93781837 9.99044995]\n"
     ]
    }
   ],
   "source": [
    "mdp = ChainMDP()\n",
    "\n",
    "N_ITERS = 10_000\n",
    "N_EVAL_ITERS = 100\n",
    "GAMMA = .9\n",
    "\n",
    "policy, V = policy_iteration(mdp, GAMMA, N_ITERS, N_EVAL_ITERS)\n",
    "print(\"POLICY ITERATION\")\n",
    "print(f\"POLICY:\\n{policy}\")\n",
    "print(f\"V\\n{V}\")\n",
    "policy, V = value_iteration(mdp, GAMMA, N_ITERS)\n",
    "print(\"VALUE ITERATION\")\n",
    "print(f\"POLICY:\\n{policy}\")\n",
    "print(f\"V\\n{V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
