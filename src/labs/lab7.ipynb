{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and Value function approximation\n",
    "\n",
    "RLDMUU, UniNE 2025, jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is the data structure specific to deep learning frameworks. They're designed to run on hardware accelerators (GPUs), making training times significantly shorter, as well as they're optimized for automatic differentiation, a central concept in deep learning. They can be initialized in following ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'list'>\n",
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "\n",
      "Data type: <class 'torch.Tensor'>\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# matrix which will serve as the base for tensor initializations\n",
    "raw_data = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "]\n",
    "\n",
    "print(f\"Data type: {type(raw_data)}\")\n",
    "print(raw_data)\n",
    "\n",
    "# creating tensor\n",
    "tensor_from_raw_data = torch.tensor(raw_data)\n",
    "\n",
    "print(f\"\\nData type: {type(tensor_from_raw_data)}\")\n",
    "print(tensor_from_raw_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can also be created from numpy arrays, retaining their attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'numpy.ndarray'>\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "Data type: <class 'torch.Tensor'>\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n",
      "\n",
      "Data type: <class 'numpy.ndarray'>\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# creating a numpy array from the data, with float datatype\n",
    "numpy_from_raw_data = np.array(raw_data, dtype=np.float64)\n",
    "\n",
    "print(f\"Data type: {type(numpy_from_raw_data)}\")\n",
    "print(numpy_from_raw_data)\n",
    "\n",
    "tensor_from_numpy = torch.from_numpy(numpy_from_raw_data)\n",
    "\n",
    "print(f\"\\nData type: {type(tensor_from_numpy)}\")\n",
    "print(tensor_from_numpy)\n",
    "\n",
    "print(f\"\\nData type: {type(tensor_from_numpy.numpy())}\")\n",
    "print(tensor_from_numpy.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single element tensors, used for example as trackers of some statistic, can be converted into numerical value by calling the `item()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'torch.Tensor'>\n",
      "tensor(9)\n",
      "\n",
      "Data type: <class 'int'>\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "single_element_tensor = torch.tensor(9)\n",
    "\n",
    "print(f\"Data type: {type(single_element_tensor)}\")\n",
    "print(single_element_tensor)\n",
    "\n",
    "print(f\"\\nData type: {type(single_element_tensor.item())}\")\n",
    "print(single_element_tensor.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in numpy, tensors with predetermined values can be created, based only on shape (passed to the function as a tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor\n",
      "tensor([[0.2106, 0.3648, 0.7391],\n",
      "        [0.5434, 0.4996, 0.2264]])\n",
      "\n",
      "Tensor of zeros\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Tensor of ones\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Tensor of arbitrary values\n",
      "tensor([[9, 9, 9],\n",
      "        [9, 9, 9]])\n",
      "\n",
      "Empty tensor\n",
      "tensor([[1.4013e-45, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "\n",
    "print(\"Random tensor\")\n",
    "print(torch.rand(shape))\n",
    "\n",
    "print(\"\\nTensor of zeros\")\n",
    "print(torch.zeros(shape))\n",
    "\n",
    "print(\"\\nTensor of ones\")\n",
    "print(torch.ones(shape))\n",
    "\n",
    "print(\"\\nTensor of arbitrary values\")\n",
    "print(torch.full(shape, 9))\n",
    "\n",
    "print(\"\\nEmpty tensor\")\n",
    "print(torch.empty(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arange vector\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "\n",
      "Linear space\n",
      "tensor([ 1.0000,  3.3333,  5.6667,  8.0000, 10.3333, 12.6667, 15.0000])\n",
      "\n",
      "Diagonal matrix\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Arange vector\")\n",
    "print(torch.arange(5))\n",
    "\n",
    "print(\"\\nLinear space\")\n",
    "print(torch.linspace(start=1, end=15, steps=7))\n",
    "\n",
    "print(\"\\nDiagonal matrix\")\n",
    "print(torch.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor is created on CPU device as a default. If CUDA (Nvidia's parallel computing platform) is available, then tensor can be moved to CUDA by `to` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n",
      "Default device: cpu\n",
      "Mapped to CUDA: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "print(f\"Default device: {tensor_from_raw_data.device}\")\n",
    "print(f\"Mapped to CUDA: {tensor_from_raw_data.to(device=device).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor can be subject to several operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([4, 5, 6])\n",
      "tensor([2, 5, 8])\n",
      "tensor([3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "print(tensor_from_raw_data)\n",
    "print(tensor_from_raw_data[1])\n",
    "print(tensor_from_raw_data[:, 1])\n",
    "print(tensor_from_raw_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 7, 2, 5, 8, 3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "# concatenation\n",
    "col1 = tensor_from_raw_data[:, 0]\n",
    "col2 = tensor_from_raw_data[:, 1]\n",
    "col3 = tensor_from_raw_data[:, -1]\n",
    "\n",
    "print(torch.cat((col1, col2, col3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 4, 7]), tensor([2, 5, 8]), tensor([3, 6, 9])]\n",
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "# stacking \n",
    "print([col1, col2, col3])\n",
    "print(torch.stack([col1, col2, col3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0702, -1.0126,  0.4220],\n",
      "         [-1.0175,  1.5008, -2.6812],\n",
      "         [-1.2802, -0.3263,  2.7129],\n",
      "         [-0.2037,  0.6597, -0.5323]],\n",
      "\n",
      "        [[ 0.9393,  0.2235, -0.1600],\n",
      "         [-0.7616,  0.6809, -1.4065],\n",
      "         [ 0.7599, -0.1478,  1.4894],\n",
      "         [-2.0196,  2.2772, -0.1568]]])\n",
      "Tensor size: torch.Size([2, 4, 3])\n",
      "tensor([[[-0.0702,  0.9393],\n",
      "         [-1.0126,  0.2235],\n",
      "         [ 0.4220, -0.1600]],\n",
      "\n",
      "        [[-1.0175, -0.7616],\n",
      "         [ 1.5008,  0.6809],\n",
      "         [-2.6812, -1.4065]],\n",
      "\n",
      "        [[-1.2802,  0.7599],\n",
      "         [-0.3263, -0.1478],\n",
      "         [ 2.7129,  1.4894]],\n",
      "\n",
      "        [[-0.2037, -2.0196],\n",
      "         [ 0.6597,  2.2772],\n",
      "         [-0.5323, -0.1568]]])\n",
      "Tensor size: torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# permuting dimensions\n",
    "x = torch.randn(2, 4, 3)\n",
    "print(x)\n",
    "print(f\"Tensor size: {x.size()}\")\n",
    "x_p = torch.permute(x, (1, 2, 0))\n",
    "print(x_p)\n",
    "print(f\"Tensor size: {x_p.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0., 0.]],\n",
      "\n",
      "          [[0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0.]],\n",
      "\n",
      "          [[0., 0.]]]]])\n",
      "Size: torch.Size([2, 1, 2, 1, 2])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]])\n",
      "Size: torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# squeezing (removing dimensions of size 1)\n",
    "x = torch.zeros(2,1,2,1,2)\n",
    "print(x)\n",
    "print(f\"Size: {x.size()}\")\n",
    "print(x.squeeze())\n",
    "print(f\"Size: {x.squeeze().size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([[1, 2, 3, 4]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "# unsqueezing (adding dimension, i.e. adding another brackets in the tensor)\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(x.size())\n",
    "print(torch.unsqueeze(x, 0))\n",
    "print(torch.unsqueeze(x, 0).size())\n",
    "print(torch.unsqueeze(x, 1))\n",
    "print(torch.unsqueeze(x, 1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# view \n",
    "print(tensor_from_raw_data.view(9))\n",
    "print(tensor_from_raw_data.view(3, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct neural network in PyTorch we have to use several building blocks available in `torch.nn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple neural network inherits from `nn.Module` class, which implements several crucial methods for neural network computations. At least `forward` method is always overwritten to declare the forward pass in the network. The structure of neural network should be specified in  `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.linear1(data))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring the instance of neural network model\n",
    "model = NeuralNetwork(input_size=16, output_size=4, hidden_dim=32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell represents the typical flow for the classification problem, where class is assigned to the one that yields the biggest output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0935, -0.0211,  0.0783,  0.0552], grad_fn=<AddBackward0>)\n",
      "Class: 0\n"
     ]
    }
   ],
   "source": [
    "# dummy input\n",
    "x = torch.rand(16, device=device)\n",
    "# scoring the model\n",
    "output = model(x)\n",
    "print(output)\n",
    "# getting probabilities via the softmax function\n",
    "probs = nn.Softmax(dim=0)(output)\n",
    "# getting the class\n",
    "choice = probs.argmax()\n",
    "print(f\"Class: {choice.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following, simple computation graph - neural networks, though much more complicated, follow the same principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](comp-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input of zeros\n",
    "x = torch.ones(5)\n",
    "# target of zeros\n",
    "y = torch.zeros(3)\n",
    "# weights and biases\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "# result to be compared against y\n",
    "z = torch.matmul(x, w) + b\n",
    "# loss function\n",
    "loss = F.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass, used to compute gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix\n",
      "tensor([[ 0.5927, -0.5245,  0.3436],\n",
      "        [ 0.1878,  0.7141, -1.3348],\n",
      "        [ 0.9460,  1.1647, -1.1685],\n",
      "        [-0.2964, -1.8670,  1.5336],\n",
      "        [-0.7341, -0.2451,  1.2785]], requires_grad=True)\n",
      "\n",
      "Weight matrix gradient\n",
      "tensor([[0.2093, 0.2431, 0.2369],\n",
      "        [0.2093, 0.2431, 0.2369],\n",
      "        [0.2093, 0.2431, 0.2369],\n",
      "        [0.2093, 0.2431, 0.2369],\n",
      "        [0.2093, 0.2431, 0.2369]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight matrix\")\n",
    "print(w)\n",
    "print(\"\\nWeight matrix gradient\")\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias vector\n",
      "tensor([-0.1728,  1.7484,  0.2459], requires_grad=True)\n",
      "\n",
      "Bias vector gradient\n",
      "tensor([0.2093, 0.2431, 0.2369])\n"
     ]
    }
   ],
   "source": [
    "print(\"Bias vector\")\n",
    "print(b)\n",
    "print(\"\\nBias vector gradient\")\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients accumulation can be turned off either by `torch.no_grad` context manager, or by `detach`, which as name suggests, detaches tensor from the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = torch.matmul(x, w) + b \n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers a wide range of optimizers, which apply the changes to parameters of the model with a given rate. One of the most popular is `Adam`, and we are going to use it throughout this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the usual flow for updating the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input data\n",
    "x = torch.rand(16, device=device)\n",
    "# dummy target\n",
    "dummy_target = torch.randn(4, device=device, dtype=torch.float)\n",
    "# Mean squared error loss - PyTorch offers a wide variety of loss functions, with the ability to declare our own \n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resets preciously accumulated gradients to 0\n",
    "optim.zero_grad()\n",
    "# scoring the model\n",
    "output = model(x)\n",
    "# applying the loss function and computing gradients with backpropagation\n",
    "loss = loss_fn(output, dummy_target)\n",
    "loss.backward()\n",
    "# applying the gradients to change the parameters\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function approximation - Semi gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is to implement the value function approximation using neural network and semi gradient TD(0) algorithm for estimating $\\hat{v} \\approx v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# declaring the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "# getting initial state\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03411342,  0.02542408,  0.01509525, -0.01545368], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring neural network which will approximate the function - technically any parametrized and differentiable function would work (with varying success probably). Feel free to design your own architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.linear1(data))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ValueFunction(4, 64).to(device)\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring $\\alpha$ and $\\gamma$ for temporal update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in Chapter 9.3 of Sutton and Barto, update for each step is given as:\n",
    "\n",
    "$w \\leftarrow w + \\alpha[R + \\gamma \\hat{v}(S', w) - \\hat{v}(S, w)]\\nabla\\hat{v}(S,w) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:48<00:00, 204.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "for _ in tqdm(range(10000)):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    output = model(torch.tensor(state).to(device))\n",
    "    loss = (alpha * (reward + gamma * model(torch.tensor(next_state).to(device)) - output))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13388.0420])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(torch.tensor([0.00, 2, -0.1, -1]).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can consider a State-action approximation, which can be updated by:\n",
    "\n",
    "$w \\leftarrow w + \\alpha[R + \\gamma \\hat{Q}(S', A', w) - \\hat{Q}(S,A,w)]\\nabla\\hat{v}(S,w) $\n",
    "\n",
    "We can assume that current policy is greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ValueFunction(5, 64).to(device)\n",
    "optim = torch.optim.Adam(params=Q.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:55<00:00, 173.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(20000)):\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(torch.cat((\n",
    "                Q(torch.cat((torch.tensor(state).to(device), torch.tensor([0.]).to(device)))),\n",
    "                Q(torch.cat((torch.tensor(state).to(device), torch.tensor([1.]).to(device))))))).item()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        state, info = env.reset()\n",
    "        continue\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_action = torch.argmax(torch.cat((\n",
    "                        Q(torch.cat((torch.tensor(next_state).to(device), torch.tensor([0.]).to(device)))),\n",
    "                        Q(torch.cat((torch.tensor(next_state).to(device), torch.tensor([1.]).to(device))))))).item()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    output = Q(torch.cat((torch.tensor(state).to(device), torch.tensor([action], dtype=torch.float).to(device))))\n",
    "    loss = (alpha * (reward + gamma *  Q(torch.cat((torch.tensor(next_state).to(device), torch.tensor([next_action], dtype=torch.float).to(device)))) - output))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    \n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- PyTorch documentation\n",
    "- Sutton, Barto. Reinforcement Learning, Chapter 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(8, requires_grad=True)\n",
    "v = lambda w: [2*w[0]+w[7], 2*w[1]+w[7], 2*w[2]+w[7], 2*w[3]+w[7], 2*w[4]+w[7], 2*w[5]+w[7], 2*w[6]+w[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2852,  0.6484, -0.6590,  0.3973,  0.1577, -0.7503,  0.6808,  0.8858],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.3155, grad_fn=<AddBackward0>),\n",
       " tensor(2.1826, grad_fn=<AddBackward0>),\n",
       " tensor(-0.4323, grad_fn=<AddBackward0>),\n",
       " tensor(1.6804, grad_fn=<AddBackward0>),\n",
       " tensor(1.2012, grad_fn=<AddBackward0>),\n",
       " tensor(-0.6148, grad_fn=<AddBackward0>),\n",
       " tensor(2.2473, grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros((7,2,7))\n",
    "P[:,0,:-1] = 1/6\n",
    "P[:, 1, 6] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros((7,2))\n",
    "policy[:, 0] = 6/7\n",
    "policy[:, 1] = 1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.random.choice(6)\n",
    "next_state = np.random.choice(7, p=P[state, np.random.choice(2, p=policy[state]), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = alpha * (0 + gamma * v(w)[next_state] - v(w)[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6000,  1.5200, -0.0400])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the $w$. Try and run this example multiple times and observe the value of $w_8$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
