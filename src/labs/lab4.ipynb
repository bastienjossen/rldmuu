{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### Backward Induction\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today your task would be to implement the backwards induction algorithm for the following MDP (you can also find it in `src/MDP/MDP.py` on our github):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## This a discrete MDP with a finite number of states and actions\n",
    "class DiscreteMDP:\n",
    "    ## initalise a random MDP with\n",
    "    ## n_states: the number of states\n",
    "    ## n_actions: the number of actions\n",
    "    ## Optional arguments:\n",
    "    ## P: the state-action-state transition matrix so that P[s,a,s_next] is the probability of s_next given the current state-action pair (s,a)\n",
    "    ## R: The state-action reward matrix so that R[s,a] is the reward for taking action a in state s.\n",
    "    def __init__(self, n_states, n_actions, P = None, R = None):\n",
    "        self.n_states = n_states # the number of states of the MDP\n",
    "        self.n_actions = n_actions # the number of actions of the MDP\n",
    "        if (P is None):\n",
    "            self.P = np.zeros([n_states, n_actions, n_states]) # the transition probability matrix of the MDP so that P[s,a,s'] is the probabiltiy of going to s' from (s,a)\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.P[s,a] = np.random.dirichlet(np.ones(n_states)) # generalisation of Beta to multiple outcome\n",
    "        else:\n",
    "            self.P = P\n",
    "        if (R is None):\n",
    "            self.R = np.zeros([n_states, n_actions]) # the expected reward for each action and state\n",
    "            # generate uniformly random transitions and 0.1 bernoulli rewards\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    self.R[s,a] = np.round(np.random.uniform(), decimals=1)\n",
    "        else:\n",
    "            self.R = R\n",
    "        \n",
    "        # check transitions\n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                #print(s,a, \":\", self.P[s,a,:])\n",
    "                assert(abs(np.sum(self.P[s,a,:])-1) <= 1e-3)\n",
    "                assert((self.P[s,a,:] <= 1).all())\n",
    "                assert((self.P[s,a,:] >= 0).all())\n",
    "                \n",
    "    # get the probability of next state j given current state s, action a, i.e. P(j|s,a)\n",
    "    def get_transition_probability(self, state, action, next_state):\n",
    "        return self.P[state, action, next_state]\n",
    "    \n",
    "    # get the vector of probabilities over next states P( . | s,a)\n",
    "    def get_transition_probabilities(self, state, action):\n",
    "        return self.P[state, action]\n",
    "    \n",
    "    # Get the reward for the current state action.\n",
    "    # It can also be interpreted as the expected reward for the state and action.\n",
    "    def get_reward(self, state, action):\n",
    "        return self.R[state, action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, in the backward induction algorithm we consider an MDP with finite horizon $T$, and for each step of the algorithm, we compute:\n",
    "\n",
    "$$ V_t(s) = max_{a \\in A} \\left[ R(s,a) + \\sum_{s' \\in S} P(s' | s,a)V_{t+1}(s') \\right]$$\n",
    "\n",
    "where $R(s,a)$ is a reward received by picking action $a$ in state $s$, $P(s'|s,a)$ is the probability of transitioning into next state $s'$, and $V_{t+1}(s')$ is the value of said next state at time $t+1$. We can also say, that for the last timestep (with index $T-1$) the next state value is 0 for every state $V_T(s) = 0$. Consecutively, the action $a$ which maximizes $V_t(s)$ can be described as policy $\\pi_t(s)$ at state $s$ and time $t$, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement this algorithm. Remember to do the inverse iteration, and iterate from $T-1$ to $0$, not the other way around. Your function should return matrix of state values for each $s$ and $t$, as well as resulting policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement backwards induction\n",
    "def backwards_induction(mdp, T):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = 5\n",
    "ACTIONS = 3\n",
    "\n",
    "T = 15\n",
    "\n",
    "mdp = DiscreteMDP(STATES, ACTIONS)\n",
    "\n",
    "V, policy = backwards_induction(mdp, T)\n",
    "\n",
    "print(V)\n",
    "\n",
    "print(policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
