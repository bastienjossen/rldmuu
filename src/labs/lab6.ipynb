{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### Q-learning and SARSA\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to implement two fundamental temporal difference algorithms - Q-learning and SARSA. Both of these algorithms choose action based on $Q(s,a)$ function, but update it in a diffetent manner. Q-learning update goes as follows:\n",
    "\n",
    "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\arg\\max_a Q(s',a) - Q(s,a)\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SARSA updates its $Q(s,a)$ in a following way:\n",
    "\n",
    "$$ Q(s,a) \\leftarrow \\alpha \\left[ r + \\gamma Q(s', a') - Q(s,a) \\right] $$\n",
    "\n",
    "where $\\alpha$ is a learning rate and $\\gamma$ is a discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between these two arises when computing the discounted value of $Q$ for next state. As $Q : S \\times A \\rightarrow \\mathbb{R}$, we need to pick the next action. We can do it either by maximizing over all actions in an off-line fashion (Q-Learning) or assume that the next action will be picked using the same poicy $\\pi$ we are currently following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to program both Q-Learning and SARSA from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rdm\n",
    "class QLearning:\n",
    "    def __init__(self, n_states, n_actions, alpha, gamma, epsilon):\n",
    "        # TODO: Initialize the class\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states,n_actions))\n",
    "        self.lr = alpha\n",
    "        self.discout = gamma\n",
    "\n",
    "    def act(self, state):\n",
    "        # TODO: pick the action\n",
    "        if rdm.uniform(0,1) <= self.epsilon:\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        else: \n",
    "            choices = np.where(np.max(self.Q[state, : ]) == self.Q[state, :])[0]\n",
    "            return np.random.choice(choices)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # TODO: Update the Q-table\n",
    "        self.Q[state,action] += self.lr *(\n",
    "            reward+self.discout *np.max(self.Q[next_state,:])-self.Q[state,action])\n",
    "        \n",
    "    def reset(self):\n",
    "        # TODO: Reset the Q-tables \n",
    "        self.Q = np.zeros((self.n_states,self.n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, n_states, n_actions, alpha, gamma, epsilon):\n",
    "        # TODO: Initialize the class\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states,n_actions))\n",
    "        self.lr = alpha\n",
    "        self.discout = gamma\n",
    "\n",
    "    def act(self, state):\n",
    "        # TODO: pick the action\n",
    "        if rdm.uniform(0,1) <= self.epsilon:\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        else: \n",
    "            choices = np.where(np.max(self.Q[state, : ]) == self.Q[state, :])[0]\n",
    "            return np.random.choice(choices)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # TODO: Update the Q-table\n",
    "        self.Q[state,action] += self.lr *(\n",
    "            reward+self.discout * self.Q[next_state,self.act(next_state)]-self.Q[state,action])\n",
    "        \n",
    "    def reset(self):\n",
    "        # TODO: Reset the Q-tables \n",
    "        self.Q = np.zeros((self.n_states,self.n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's introduce a python framework that you are going to work with over the course of this semester, namely `gymnasium`, which is the successor of OpenAI `gym`. Let's go through the basic functionality of `gymnasium` based environments. First, let's import a Frozen Lake environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the first use, and after each episode we have to reset an environment. `reset()` function returns the state represenation and an additional dictionary `info`, if we ever wanted to collect some additional data about the environment. For now we won't take it into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "# useful for checking if the environment terminated\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: In order to create Q tables we have to know the size of the state and action space. We can check it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space size: Discrete(16)\n",
      "Action space size: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation space size: {env.observation_space}\")\n",
    "print(f\"Action space size: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's act on the environment and observe the results (we sample the action for now). For this reason we provide an action to the `step` method and observe the following:\n",
    "\n",
    "- `next_state` to which we transition\n",
    "- `reward` received\n",
    "- `done` signal, indicating if the environment terminated\n",
    "- `truncated` signal, indicating whether a timeout or other external constraint had been reached\n",
    "- `info` dict with supplementary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "next_state, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to perform both Q-Learning and SARSA to learn the optimal policy for an agent acting in an environment. After you're done, plot the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODE = 1000\n",
    "N_ITER = 1000\n",
    "ALPHA = 1e-2\n",
    "GAMMA = 0.5\n",
    "epsilon = 1e-1\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "done = False\n",
    "\n",
    "algorithm = QLearning(env.observation_space.n,\n",
    "                      env.action_space.n,\n",
    "                      alpha=ALPHA,\n",
    "                      gamma=GAMMA,\n",
    "                      epsilon=epsilon)\n",
    "\n",
    "nsteps = np.zeros(N_EPISODE) * N_ITER\n",
    "for e in range(N_EPISODE):\n",
    "    for i in range(N_ITER):\n",
    "        action = algorithm.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        # truncated = not done but stuck.\n",
    "        #Â done is won or died\n",
    "        # info is every other information besides the returned ones.\n",
    "\n",
    "        algorithm.update(state, action, reward, next_state)\n",
    "\n",
    "        if done or truncated:\n",
    "            state, info = env.reset()\n",
    "            done = False\n",
    "            truncated = False\n",
    "            if reward == 1:\n",
    "                nsteps[e] = i #Save the amount of steps to win\n",
    "            break\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(nsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "\n",
    "done = False\n",
    "\n",
    "algorithm = SARSA(env.observation_space.n,\n",
    "                      env.action_space.n,\n",
    "                      alpha=ALPHA,\n",
    "                      gamma=GAMMA,\n",
    "                      epsilon=epsilon)\n",
    "\n",
    "nsteps = np.zeros(N_EPISODE) * N_ITER\n",
    "for e in range(N_EPISODE):\n",
    "    for i in range(N_ITER):\n",
    "        action = algorithm.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        # truncated = not done but stuck.\n",
    "        #Â done is won or died\n",
    "        # info is every other information besides the returned ones.\n",
    "\n",
    "        algorithm.update(state, action, reward, next_state)\n",
    "\n",
    "        if done or truncated:\n",
    "            state, info = env.reset()\n",
    "            done = False\n",
    "            truncated = False\n",
    "            if reward == 1:\n",
    "                nsteps[e] = i #Save the amount of steps to win\n",
    "            break\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(nsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
