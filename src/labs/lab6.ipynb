{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### Q-learning and SARSA\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to implement two fundamental temporal difference algorithms - Q-learning and SARSA. Both of these algorithms choose action based on $Q(s,a)$ function, but update it in a diffetent manner. Q-learning update goes as follows:\n",
    "\n",
    "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\arg\\max_a Q(s',a) - Q(s,a)\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SARSA updates its $Q(s,a)$ in a following way:\n",
    "\n",
    "$$ Q(s,a) \\leftarrow \\alpha \\left[ r + \\gamma Q(s', a') - Q(s,a) \\right] $$\n",
    "\n",
    "where $\\alpha$ is a learning rate and $\\gamma$ is a discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between these two arises when computing the discounted value of $Q$ for next state. As $Q : S \\times A \\rightarrow \\mathbb{R}$, we need to pick the next action. We can do it either by maximizing over all actions in an off-line fashion (Q-Learning) or assume that the next action will be picked using the same poicy $\\pi$ we are currently following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to program both Q-Learning and SARSA from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, n_states, n_actions, alpha, gamma, epsilon):\n",
    "        # TODO: Initialize the class\n",
    "        pass \n",
    "\n",
    "    def act(self):\n",
    "        # TODO: pick the action\n",
    "        pass\n",
    "\n",
    "    def update(self, action, reward, next_state):\n",
    "        # TODO: Update the Q-table\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO: Reset the Q-tables \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, n_states, n_actions, alpha, gamma, epsilon):\n",
    "        # TODO: Initialize the class\n",
    "        pass \n",
    "\n",
    "    def act(self):\n",
    "        # TODO: pick the action\n",
    "        pass\n",
    "\n",
    "    def update(self, action, reward, next_state):\n",
    "        # TODO: Update the Q-table\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO: Reset the Q-tables \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's introduce a python framework that you are going to work with over the course of this semester, namely `gymnasium`, which is the successor of OpenAI `gym`. Let's go through the basic functionality of `gymnasium` based environments. First, let's import a Frozen Lake environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the first use, and after each episode we have to reset an environment. `reset()` function returns the state represenation and an additional dictionary `info`, if we ever wanted to collect some additional data about the environment. For now we won't take it into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "# useful for checking if the environment terminated\n",
    "done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: In order to create Q tables we have to know the size of the state and action space. We can check it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space size: Discrete(16)\n",
      "Action space size: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation space size: {env.observation_space}\")\n",
    "print(f\"Action space size: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's act on the environment and observe the results (we sample the action for now). For this reason we provide an action to the `step` method and observe the following:\n",
    "\n",
    "- `next_state` to which we transition\n",
    "- `reward` received\n",
    "- `done` signal, indicating if the environment terminated\n",
    "- `truncated` signal, indicating whether a timeout or other external constraint had been reached\n",
    "- `info` dict with supplementary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "next_state, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to perform both Q-Learning and SARSA to learn the optimal policy for an agent acting in an environment. After you're done, plot the rewards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
