Course name: Reinforcement Learning and Decision Making Under Uncertainty

The course will give a thorough introduction to reinforcement learning. The first 8 weeks will be devoted to the core theory and algorithms of reinforcement learning. The final 6 weeks will be focused on project work, during which more advanced topics will be inroduced. 

The first 6 weeks will require the students to complete 3 assignments. The remainder of the term, the students will have to prepare a project, for which they will need to submit a report.



* Schedule

|----+--------------------------------------------------|
|  1 | Beliefs and Decisions                            |
|----+--------------------------------------------------|
|  2 | Bayesian Decision Rules                          |
|----+--------------------------------------------------|
|  3 | Estimation Theory and Concentration Inequalities |
|----+--------------------------------------------------|
|  4 | Bandit problems.                                 |
|    | MDPs, belief states, Backwards Induction.        |
|----+--------------------------------------------------|
|  5 | MDP Theory: Value Iteration                      |
|    | Policy Iteration                                 |
|----+--------------------------------------------------|
|  6 | Sarsa / Q-Learning                               |
|----+--------------------------------------------------|
|  7 | Actor-Critic / Model-Based RL                    |
|----+--------------------------------------------------|
|  8 | Function Approximation, Gradient Methods         |
|----+--------------------------------------------------|
|  9 | Bayesian RL                                      |
|----+--------------------------------------------------|
| 10 | UCB, Regret bounds                               |
|    | UCRL, Regret bounds                              |
|----+--------------------------------------------------|
| 11 | Large-scale RL.                                  |
|    | UCT/AlphaZero etc.                               |
|----+--------------------------------------------------|
| 11 | Advanced Bayesian models                         |
|----+--------------------------------------------------|
| 12 | Inverse Reinforcement Learning                   |
|----+--------------------------------------------------|
| 13 | Multiagent extensions: Bayesian Games            |
|----+--------------------------------------------------|
| 14 | Group work                                       |
|----+--------------------------------------------------|
** Beliefs and decisions
*** Probability primer (45')
1. Objective vs Subjective Probability: Example (5')
2. Relative likelihood: Completeness, Consistency, Transitivity, Complement, Subset (5')
3. Measure theory (5')
4. Axioms of Probability (5')
5. Random variables (5')
6. Expectations (5')
7. Expectations Exercise (10')

1. 
- Quantum Physics
- Coin toss

2. Relative Likelihood

Completeness A>B, B>A or A=B for any A,B
Transitivity A>B, B>C, A>C
Complement: A>B => ~A<~B
Subset: $A \subset B \Rightarrow A < B$

3. Measure theory 

We can use probability to quantify this, so that
$A > B$ iff $P(A) > P(B)$.
But what do we mean by this?

Measure as a concept: area, length, probability
$M(A) + M(B) = M(A \cup B)$

4. Axioms of Probability
$P : \Sigma \to [0,1]$
$P(\emptyset) = 0$
$P(\Omega) = 1$
If $A \cap B = \emptyset$, $P(A \cup B) = P(A) + P(B)$.

5. Exercise: Prove that P satisfies the given properties

6. Random variables

$f: \Omega \to \Reals$

7. Expectations



*** Utility theory (45')
1. Rewards and preferences
2. Transitivity of preferences
3. Random rewards
4. Utility functions and the expected utility hypothesis
5. Utility exercise: Gambling (10' pen and paper)
6. The St. Petersburg Paradox

*** Lab: Probability, Expectation, Utility

1. Exercise Set 1. Probability introduction.
2. Exercise Set 2. Sec 2.4, Exercises 4, 5, 11, 12.


The utility analysis of choices involving risk:
https://www.journals.uchicago.edu/doi/abs/10.1086/256692


The expected-utility hypothesis and the measurability of utility
https://www.journals.uchicago.edu/doi/abs/10.1086/257308


** Decisions with observations
*** Problems with Observations (45')
1. Conditional probability (5')
2. Bayes theorem (10')
3. Discrete set of models example: the meteorologists problem (30')

*** Basic decisions (45')

1. Linearity of Expectations (5')
2. Convexity of Bayes Decisions (5')
3. Game theory (10')
4. Linear programming for games (10')

*** Lab: Decision problems and estimation

1. Problems with no observations. Exercise: 1, 2, 3a.
2. Problems with observations. Exercises 4,5

*** Assignment. An insurance problem

Exercise Set 4.


** Bayesian Analysis and Estimation Theory
- Conjugate priors (15')
- Beta Example  (15')
- Normal Example  (15')
- Bayesian estimation (15')
- Chernoff Bounds and Concentration Inequalities (15')
- Hypothesis testing

*** Concentration lab
- Construction of credible intervals (15')
- Concentration inequalities (15')
- Hypothesis testing (15')

Focus on the beta-bernoulli model and hypothesis testing.


** Bandit problems

1. n-meteorologists vs partial information (15')
2. Stochastic bandit problems (15')
3. A simple algorithm: the belief (15')
4. The optimisation problem (15')
5. Backwards induction (15')
6. Backwards induction example (15')


** Markov Decision Processes: Finite horizon

1. MDP definitions (15')
2. MDP examples (15')
3. The bandit MDP (15')
4. Monte Carlo Policy Evaluation (15')
5. DP: Finite Horizon Policy Evaluation (15')
6. DP: Finite Horizon Backward Induction (15')

** Markov Decision Processes: Infinite horizon I

1. DP: Value Iteration (45')
2. DP: Policy Iteration (45')

** Markov Decision Processes: Infinite horizon II

1. DP: Temporal Differences (45')
2. DP: Modified Policy Iteration (45')

** Markov Decision Processes: Stochastic Approximation

1. Sarsa (45')
2. Q-learning (45')
 
** Model-based RL
1. Actor-Critic Algorithms (45')
2. Model-based RL (45')

** Large and continuous state spaces
1. Function approximationm (45')
2. Gradient methods (45')

** Bayesian methods

1. Thompson sampling (25')
2. Bayesian Policy Gradient (20')
3. BAMDPs (25')
4. POMDPs (20')

** Regret bounds

1. UCB (45')
2. UCRL (45')

** MCTS

1. UCT (45')
2. Alphazero (45')

** Advanced Bayesian Models

1. Linear Models (20')
2. Gaussian Processes (25')
3. GPTD (45')

** Inverse Reinforcment Learning

1. Apprenticeship learning (45')
2. Probabilistic IRL (45')

** Multiplayer games

Bayesian games (90')

   
