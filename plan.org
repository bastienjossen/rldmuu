#+TITLE: Reinforcement Learning and Decision Making Under Uncertainty
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \bel {\xi}
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

Course name: 

The course will give a thorough introduction to reinforcement
learning. The first 8 weeks will be devoted to the core theory and
algorithms of reinforcement learning. The final 6 weeks will be
focused on project work, during which more advanced topics will be
inroduced.

The first 6 weeks will require the students to complete 5
assignments. The remainder of the term, the students will have to
prepare a project, for which they will need to submit a report.



* Schedule

|------+--------------------------------------------|
| Week | Topic                                      |
|------+--------------------------------------------|
|    1 | Beliefs and Decisions                      |
|------+--------------------------------------------|
|    2 | Bayesian Decision Rules                    |
|------+--------------------------------------------|
|    3 | Introduction to Bandit problems.           |
|------+--------------------------------------------|
|    4 | Finite Horizon MDPs                        |
|      | Backwards Induction                        |
|      | The Bandit MDP                             |
|------+--------------------------------------------|
|    5 | Infite Horizon MDPs                        |
|      | Value Iteration                            |
|      | Policy Iteration                           |
|------+--------------------------------------------|
|    6 | Sarsa / Q-Learning                         |
|------+--------------------------------------------|
|    7 | Model-Based RL                             |
|------+--------------------------------------------|
|    8 | Function Approximation, Gradient Methods   |
|------+--------------------------------------------|
|    9 | Bayesian RL: Dynamic Programming, Sampling |
|------+--------------------------------------------|
|   11 | UCB/UCRL/UCT.                              |
|      | UCT/AlphaZero.                             |
|------+--------------------------------------------|
|   12 | Inverse Reinforcement Learning             |
|------+--------------------------------------------|
|   13 | Multiagent extensions: Bayesian Games      |
|------+--------------------------------------------|
|   13 | Project presnetations                      |
|------+--------------------------------------------|
|   14 | Q&A, Mock exam                             |
|------+--------------------------------------------|
** Beliefs and decisions
*** Utility theory (90')
1. Rewards and preferences (15') 
2. Transitivity of preferences (15')
3. Random rewards (5')
4. Decision Diagrams (10')
5. Utility functions and the expected utility hypothesis (15')
6. Utility exercise: Gambling (10' pen and paper)
7. The St. Petersburg Paradox (15')
   
*** Probability primer
1. Objective vs Subjective Probability: Example (5')
2. Relative likelihood: Completeness, Consistency, Transitivity, Complement, Subset (5')
3. Measure theory (5')
4. Axioms of Probability (5')
5. Random variables (5')
6. Expectations (5')
7. [[file:src/beliefs_and_decisions/probability.py][Expectations exercise]] (10')

1. 
- Quantum Physics
- Coin toss

2. Relative Likelihood

Completeness A>B, B>A or A=B for any A,B
Transitivity A>B, B>C, A>C
Complement: A>B => ~A<~B
Subset: $A \subset B \Rightarrow A < B$

3. Measure theory 

We can use probability to quantify this, so that
$A > B$ iff $P(A) > P(B)$.
But what do we mean by this?

Measure as a concept: area, length, probability
$M(A) + M(B) = M(A \cup B)$

4. Axioms of Probability
$P : \Sigma \to [0,1]$
$P(\emptyset) = 0$
$P(\Omega) = 1$
If $A \cap B = \emptyset$, $P(A \cup B) = P(A) + P(B)$.

5. Exercise: Prove that P satisfies the given properties

6. Random variables

If $\omega$ is distributed according to $P$, then the function $f(\omega)$, with 
$f: \Omega \to \Reals$, is a /random variable/ with distribution $P_f$, where:
\[
P_f(A) = P(\{\omega : f(\omega) \in A\})
\]

7. Expectations

$E_P[f] = \sum_{\omega} f(\omega) P(\omega)$.

*** Lab: Probability, Expectation, Utility

1. Exercise Set 1. Probability introduction.
2. Exercise Set 2. Sec 2.4, Exercises 4, 5.

*** Assignment.

Exercise 7, 8, 9.

*** Seminar:

Utility. What is the concept of utility? Why do we want to always maximise utility?

Example:

|----+----+----|
| U  | w1 | w2 |
|----+----+----|
| a1 |  4 |  1 |
| a2 |  3 |  3 |
|----+----+----|
  
Regret. Alternative notion.

|----+----+----|
| L  | w1 | w2 |
|----+----+----|
| a1 |  0 |  2 |
| a2 |  1 |  0 |
|----+----+----|

Minimising regret is the same as maximising utility when w does not depend on a.
Hint: So that if $E[L|a^*] \leq E[L|a]$ for all $a'$, $E[U|a^*] \geq E[L|a]$ for all $a'$,

The utility analysis of choices involving risk:
https://www.journals.uchicago.edu/doi/abs/10.1086/256692


The expected-utility hypothesis and the measurability of utility
https://www.journals.uchicago.edu/doi/abs/10.1086/257308

** Decisions with observations
*** Problems with Observations (45')
1. Discrete set of models example: the meteorologists problem (25')
2. Marginal probabilities (5').
3. Conditional probability (5').
4. Bayes theorem (10').

*** Statistical decisions (45')
1. ML Estimation (10')
2. MAP Estimation (10')
3. Bayes Estimation (10')
4. MSE Estimation (10') [not done]
5. Linearity of Expectations (10') [not done]
6. Convexity of Bayes Decisions (10') [not done]

*** Lab: Decision problems and estimation (45')

1. Problems with no observations. Book Exercise: 13,14,15.
2. Problems with observations. Book Exercise: 17, 18.

*** Assignment: James Randi

** Bandit problems

*** $n$ meteorologists as prediction with expert advice

   - Predictions $p_t= p_{t,1}, \ldots, p_{t,n}$ of all models for outcomes $y_t$
   - Make decision $a_t$.
   - Observe true outcome $y_t$
   - Obtain instant reward $r_t = \rho(a_t, y_t)$
   - Utility $U = \sum_{t=1}^T r_t$.
   - $T$ is the problem *horizon*

**** At each step $t$:
1. Observe $p_t$.
2. Calculate $\hat{p}_t = \sum_\mu \xi_{t}(\mu) p_{t,\mu}$
3. Make decision $a_t = \argmax_a \sum_{y} \hat{p}_t(y) \rho(a, y)$.
4. Observe $y_t$ and obtain reward $r_t = \rho(a_t, y_t)$.
5. Update: $\xi_{t+1}(\mu) \propto \xi_t(\mu) p_{t,\mu}(y_t)$.

The update *does not depend* on $a_t$

*** Prediction with expert advice

   - Advice $p_t= p_{t,1}, \ldots, p_{t,n} \in D$ 
   - Make prediction $\hat{p}_t \in D$
   - Observe true outcome $y_t \in Y$
   - Obtain instant reward $r_t = u(\hat{p}_t, y_t)$
   - Utility $U = \sum_{t=1}^T r_t$.

**** Relation to $n$ meteorologists
- $D$ is the set of distributions on $Y$.
- However, there are only predictions, no actions. To add actions:
\[
u(\hat{p}_t, y_t) = \rho(a^*(\hat{p}_t), y_t),
\qquad
a^*(\hat{p}_t) = \argmax_a \rho(a, y_t)
\]

The update *does not depend* on $a_t$




*** The Exponentially Weighted Average

**** MWA Algorithm
- Predict by averaging all of the predictions:
\[
\hat{p}_t(y) = \sum_{\mu} \bel_t(\mu)  p_{t,\mu}(y)
\]
- Update by weighting the quality of each prediction
\[
\bel_{t+1}(\mu)
=
\frac{\bel_t(\mu) \exp[\eta u(p_{t, \mu }, y_t)]}{\sum_{\mu'} \bel_t(\mu') \exp[\eta u(p_{t,\mu}, y_t)]}
\]
**** Choices for $u$
- $u(p_{t,\mu}, y_t) = \ln p_{t,\mu}(y_t)$, $\eta = 1$, Bayes's theorem.
- $u(p_{t,\mu}, y_t) = \rho(a^*(p_{t,\mu}), y_t)$: quality of expert prediction.

*** The $n$ armed stochastic bandit problem
- Take action $a_t$
- Obtain reward $r_t \sim P_{a_t}(r)$ with expected value $\mu_{a_t}$.
- The utility is $U = \sum_t r_t$, while $P$ is *unknown*.

**** The Regret
-Total regret with respect to the best arm:
\[
L \defn \sum_{t = 1}^T [\mu^* - r_t],
\qquad
\mu^* = \max_a \mu_a
\]
- Expected regret of an algorithm $\pi$:
\[
\E^\pi [L] = \sum_{t = 1}^T \E^\pi[\mu^* - r_t],
= \sum_{a=1}^n \E^\pi[n_{T,a}](\mu^* - \mu_a)
\]
- $n_{T,a}$ is the number of times $a$ has been pulled after $n$ steps.

*** Bernoulli bandits
A classical example of this is when the rewards are Bernoulli, i.e.
\[
r_t | a_t = i \sim \textrm{Bernoulli}(\mu_i)
\]

**** Greedy algorithm
- Take action $a_t = \argmax_a \hat{\mu}_{t,a}$
- Obtain reward $r_t \sim P_{a_t}(r)$ with expected value $\mu_{a_t}$.
- Update arm: $s_{t, a_t} = s_{t - 1, a_t} + r_t$, $n_{t, a_t} = n_{t - 1, a_t} + 1$.
- Others stay the same:  $s_{t,a} = s_{t-1, a}$, $n_{t,a} = n_{t-1, a}$ for $a \neq a_t$.
- Update means: $\hat{\mu}_{t,i} = s_{t,i} / n_{t,i}$.
  

*** Policies and exploration

- $n_{t,i}, s_{t,i}$ are *sufficient statistics* for Bernoulli bandits.
- The more often we pull an arm, the more certain we are the mean is correct.
**** Upper confidence bound: exploration bonuses
- Take action $a_t = \argmax_a \hat{\mu}_{t,a} + O(1/\sqrt{n_{t,a}})$.
**** Posterior sampling: randomisation
- Given some prior parameters $\alpha, \beta > 0$ (e.g. 1).
- $\bel_t(\mu_a) = \textrm{Beta}(\alpha + s_{t,a}, \beta + n_{t,a} - s_{t,a})$.
- Sample $\hat{\mu} \sim \bel_t(\mu)$.
- Take action $a_t = \argmax_a \hat{\mu}_a$.

*** The upper confidence bound
Let
\[
\hat{\mu}_n = \sum_{i=1}^t r_i / n,
\]
be the sample mean estimate of an iiid RV in [0,1] with $\E[r_i] = \mu$. Then we have
\[
\Pr(\hat{\mu}_n \geq \mu + \epsilon) \leq \exp(-2n\epsilon^2)
\]
or equivalently
\[
\Pr(
\hat{\mu}_n \geq \mu_n + \sqrt{\ln(1/\delta)/2n} \leq \delta.
)
\]
	
*** Beta distributions as beliefs


-   [Go through Chapter 4, Beta distribution]
-  [Visualise Beta distribution]
-   [Do the James Random Exercise 3]
  
-   Note that the problem here is that this is only a point estimate: it ignores uncertainty. In fact, we can represent our uncertainty about the arms in a probabilistic way with the Beta distribution:

  If our prior over an arm's mean is $\textrm{Beta}(\alpha, \beta)$ then the -posterior at time $t$ is $\textrm{Beta}(\alpha + s_{t,i}, \beta + n_{t,i} - s_{t,i})$.

-  [Visualise how the posterior changes for a biased coin as we obtain more data].
  

*** Assignment and exercise

1. Implement epsilon-greedy bandits (lab, 30')
2. Implement Thompson sampling bandits (lab, 30')
3, Implement UCB bandits (home)
4. Compare them in a benchmark (home)

** Markov Decision Processes: Finite horizon

1. MDP definitions (15')
2. MDP examples (15')
3. The bandit MDP (15')
4. Monte Carlo Policy Evaluation (15')
5. DP: Finite Horizon Policy Evaluation (15')
6. DP: Finite Horizon Backward Induction (15')

*** The Markov decision process
**** Interaction at time $t$
- Observe state $s_t \in S$
- Take action $a_t \in A$.
- Obtain reward $r_t \in \Reals$.
**** The MDP model $\mu$
- Transition kernel $P_\mu(s_{t+1} | s_t, a_t)$.
- Reward with mean $\rho_\mu(s_t, a_t)$
**** Policies
- Markov policies $\pol(a_t | s_t)$
**** Utility
Total reward up to a finite (but not necessarily fixed) horizon $T$
\[
U = \sum_{t=1}^T r_t
\]

*** MDP examples
**** Shortest path problems
- Goal state $s^* \in S$.
- Reward $r_t = -1$ for all $s \neq s^*$
- Game ends time $T$ where $s_T = s^*$.

**** Blackjack against a croupier
- Croupier shows one card.
- Current state is croupier's card and your cards.
- Reward is $r_T = 1$ if you win, $r_T = -1$ if you lose at the end, otherwise $0$.



** Markov Decision Processes: Infinite horizon I

1. DP: Value Iteration (45')
2. DP: Policy Iteration (45')

** Markov Decision Processes: Infinite horizon II

1. DP: Temporal Differences (45')
2. DP: Modified Policy Iteration (45')

** Markov Decision Processes: Stochastic Approximation

1. Sarsa (45')
2. Q-learning (45')
 
** Model-based RL
1. Actor-Critic Algorithms (45')
2. Model-based RL (45')
** Approximate Dynamic Programming
1. Fitted Value Iteration (45')
2. Approximate Policy Iteration (45')
** Policy Gradient
1. Direct Policy Gradient, i.e. REINFORCE (45')
2. Actor-Critic Methods, e.g. Soft Actor Critic (45')
** Bayesian methods
1. Thompson sampling (25')
2. Bayesian Policy Gradient (20')
3. BAMDPs (25')
4. POMDPs (20')

** Regret bounds
1. UCB (45')
2. UCRL (45')
** MCTS
1. UCT (45')
2. Alphazero (45')
** Advanced Bayesian Models
1. Linear Models (20')
2. Gaussian Processes (25')
3. GPTD (45')

** Inverse Reinforcment Learning

1. Apprenticeship learning (45')
2. Probabilistic IRL (45')

** Multiplayer games

Bayesian games (90')

   
