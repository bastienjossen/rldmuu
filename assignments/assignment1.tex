\documentclass[twoside,a4paper]{article}
% \usepackage{enumerate}
% \setbeamertemplate{enumerate item}{(\alph{enumi})}
% \setbeamertemplate{enumerate subitem}{(\roman{enumii})}

\usepackage{xfrac}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{isomath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathrsfs}
\usepackage{dsfont}
\usepackage{epic}
\usepackage{pstool}


\input{preamble}


\pagestyle{myheadings}


\def \solution {1}
\begin{document}
\title{Exercise set 1}
\author{Christos Dimitrakakis: \texttt{christos.dimitrakakis@unine.ch}}

\maketitle
\large{Deadline: 28 February 2023}

\vspace{1em}

\section{In-class exercises}
\begin{exercise}
  Case 1:
  \begin{enumerate}
  \item The reward is 500,000 with certainty.
  \item The reward is 2,500,000 with probability $0.10$. It is 500,000 with probability $0.89$, and 0 with probability $0.01$.
  \end{enumerate}

  Case 2:
  \begin{enumerate}
  \item The reward is 500,000 with probability $0.11$, or 0 with probability $0.89$.
  \item The reward is 2,500,000 with probability $0.1$, or 0 with probability $0.9$.
  \end{enumerate}

  Show that under the expected utility hypothesis, if gamble 1 is preferred in Case 1, gamble 1 must also be preferred in the Case 2, for any increasing utility function.
\end{exercise}

\begin{exercise}[15]
  Consider the St. Petersburg paradox, where a coin is tossed until it comes heads. 
  We have estabilshed that if your utility for money is $U(x) = x$, and the coin was fair, i.e. the probability of heads was $1/2$
  then you should be willing to pay any amount to play the game. In this exercise, let us consider two alternative hypotheses:
  \begin{itemize}
  \item  Assume that the coin is fair, but your utility for money is $U(x) = \max \{0, \ln x\}$. How much
    would you now be willing to pay to play the game? Hint: Calculate the expected utility of playing.
  \item Assume that the coin is not fair, but it comes heads with probability 0.6, and your
    utility is linear: $U(x) = x$. How much would you be willing to play now?
  \item Perform an experiment to validate your claims using the code in \verb!src/beliefs_and_decisions/paradox.ipynb!
  \end{itemize}
\end{exercise}


\section{Homework}
(If you are not done with the in-class exercises, finish them at home)

\begin{exercise}
  Assuming that $U$ is increasing and absolutely continuous, consider the following experiment:
  \begin{enumerate}
  \item You specify an amount $a$, then observe random value $Y$.
  \item If $Y \geq a$, you receive $Y$ currency units.
  \item If $Y < a$, you receive a random amount $X$ with known distribution (independent of~$Y$).
  \end{enumerate}
  Show that we should choose $a$ such that $U(a) = \E[U(X)]$.
\end{exercise}

\if\solution 1
\begin{proof}[Exercise 3 solution]
  Let $\mu = \E[U(X)]$.  From the problem definition, the expected
  utility of choosing $a$ such that $U(a) = \mu$ is:
  \begin{align*}
    \E(U \mid a)
    & = 
    \int_a^\infty U(y) \dd{P_Y}(y) 
    + 
    \mu \int_{-\infty}^a \dd{P_Y}(y).
  \end{align*}
  Let some $\epsilon >
  0$. Now select $a - \epsilon$:
  \begin{align*}
    \E(U \mid a - \epsilon)
    & = 
    \int_{a - \epsilon}^\infty U(y) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^{a-\epsilon} \dd{P_Y}(y)
    \\
    & = 
    \int_a^\infty U(y) \dd{P_Y}(y) 
    + 
    \int_{a-\epsilon}^a U(y) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^{a-\epsilon} \dd{P_Y}(y)
  \end{align*}
  Since $U(a) = \mu$ and $U$ is increasing, $U(y) \leq
  \mu$ for all $y < a$. Consequently:
  \begin{align*}
    \E(U \mid a - \epsilon)
    & \leq
    \int_a^\infty U(y) \dd{P_Y}(y) 
    + 
    \int_{a-\epsilon}^a U(a) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^{a - \epsilon} \dd{P_Y}(y)
    \\
    & =
    \int_a^\infty U(y) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^a \dd{P_Y}(y)
    =
    \E(U \mid a).
  \end{align*}
  Similarly, if we select $a + \epsilon$, we have:
  \begin{align*}
    \E(U \mid a + \epsilon)
    & = 
    \int_{a+\epsilon} ^\infty U(y) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^{a + \epsilon} \dd{P_Y}(y)
    \\
    & = 
    \int_{a+\epsilon} ^\infty U(y) \dd{P_Y}(y) 
    +
    \int_{a}^{a + \epsilon} U(a) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^{a} \dd{P_Y}(y)
    \\
    &\leq
    \int_{a+\epsilon} ^\infty U(y) \dd{P_Y}(y) 
    +
    \int_{a}^{a + \epsilon} U(y) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^{a} \dd{P_Y}(y)
    \\
    &=
    \int_{a} ^\infty U(y) \dd{P_Y}(y) 
    +
    \mu \int_{-\infty}^{a} \dd{P_Y}(y)
    = 
    \E(U \mid a).
  \end{align*}
  An alternative proof may be obtained by applying the fundamental theorem of calculus:
  \[
  \dd/\dd x \left(\int_a^x f(t)\dd{t} \right)  = f(x)
  \]
  to show that the derivative of $\E(U \mid a)$ with respect to $a$ is zero at $U(a) = \mu$:
  \begin{align*}
    \E[U \mid a]
    &=
      \E[U \mid Y \geq a] \Pr(Y \geq a)
      +
      \E[U \mid Y < a] \Pr(Y < a)
    \\
    &=
      \int_{a}^\infty U(y) p_Y(y) \dd y
      +
      \mu \int_{-\infty}^a p_Y(y) \dd y
  \end{align*}
  Taking derivatives wrt $a$:
  \begin{align*} 
    \dd \E[U \mid a]/\dd a
    &=
      \frac{\dd}{\dd a} \int_{a}^\infty U(y) p_Y(y) \dd y
      +
      \frac{\dd}{\dd a} \mu \int_{-\infty}^a p_Y(y) \dd y
    \\
    &=
      - U(a) p_Y(a) 
      +
      \mu p_Y(a).
  \end{align*}
  Solving for $\dd \E[U \mid a]/\dd a=0$ gives $U(a) = \mu$.
\end{proof}
\fi
\begin{exercise}[usefulness of probability and utility]
  \begin{enumerate}
    \item Would it be useful to separate randomness from uncertainty? What would be desirable properties of an alternative concept to probability?
  \item Give an example of how the expected utility assumption might be violated.
  \end{enumerate}
\end{exercise}
\if\solution 1
\begin{proof}[Exercise 2 solution]
  I am not sure if it is useful, but it is certainly possible. We would like some function $\xi(\omega)$ telling us how sure we are that some $\omega$ is true. What values could this take? It could be in a bounded range, like probability. It could have multiple values. For example, it might return two values, an upper and lower bound on the likelihood of an event.


  What about properties? I think one fundamental property is conditioning on information. If we start with some initial belief $\xi(\cdot)$, we would like to be able to calculate $\xi(\cdot | D)$ given some data $D$ in a consistent manner.

  A real-world example is not possible: it is easy to simply have an arbitrary utility function, that can explain arbitrary individual behaviours. However, in practice we can assume that people have ``similar'' utility functions, and so statistically infer that their aggregate behaviour may not correspond to maximising expected utility.

  Mathematically, consider the first exercise. If somebody says they prefer A to B in the first case but B to A in the second case, then the expected utility hypothesis is indeed violated.
\end{proof}
\fi

\section{Feedback (to be answered in ILIAS)}


\begin{exercise}[5]
  Finally, some questions about today's unit
  \begin{enumerate}
  \item Did you find the material interesting?
  \item Did you find it potentially useful?
  \item How much did you already know?
  \item How much had you already seen but did not remember in detail?
  \item How much have you seen for the first time?
  \item Which aspect did you like the most?
  \item Which aspect did you like the least?
  \item Did the exercises help you in understanding the material?
  \item Were the exercises necessary to understand the material?
  \item Feel free to add any further comments.
  \end{enumerate}
\end{exercise}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
