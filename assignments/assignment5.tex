\documentclass[twoside,a4paper]{article}
\usepackage[notheorems]{beamerarticle}
\usepackage{listings}
\usepackage{color}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{psfrag}
\usepackage{url}
\usepackage{listings}
 \usepackage{enumerate}
\usepackage{xr}


% \setbeamertemplate{enumerate item}{(\alph{enumi})}
% \setbeamertemplate{enumerate subitem}{(\roman{enumii})}
\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{ %
  language=Octave,                % the language of the code
  basicstyle=\small,           % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=2,                   % the step between two line-numbers. If it's 1, each line 
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},      % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                   % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},          % keyword style
  commentstyle=\color{dkgreen},       % comment style
  stringstyle=\color{mauve},         % string literal style
  escapeinside={\%*}{*},            % if you want to add LaTeX within your code
  morekeywords={*,...},              % if you want to add more keywords to the set
  deletekeywords={...}              % if you want to delete keywords from the given language
}

\input{preamble}


\pagestyle{myheadings}
\markboth{Exercise set 5}{Reinforcement Learning and Decision Making under Uncertainty}

%\newcommand\sinc{\mathop{\rm sinc}}

\def\solution {1}

\begin{document}
\title{Assignment 5}
\author{Christos Dimitrakakis  \texttt{christos.dimitrakakis@unine.ch}}
\date{Deadline: 7 April 2022}
\maketitle

The purpose of this exercise is to see how different algorithms converge with different amounts of data, and different hyperparameters.

For this exercise, implement either QLearning or Sarsa. Using any OpenAI Gym environment,\footnote{As an example, use Chain, with a discount factor of $\gamma = 0.95$.} perform the following experiments:
\begin{itemize}
\item An important parameter is the step size  $\alpha_n$, which affects the convergence of the value function estimate. How easy is it to set this parameter? In particular, plot the average reward and/or utility of the policy as you obtain more data.
\item Another important parameter is how to choose the policy of the agent. Simply choosing the action maximising the $Q$ function risks the agent getting stuck with a sub-optimal policy. The simplest idea is to use epsilon-greedy action selection  with parameter $\epsilon_t$ slowly tending to zero. Investigate the effect of this parameter on how quickly the optimal policy can be found.
\item Q-Learning and Sarsa are typically implemented by online updating. For every observed $(s_t,a_t, r_t, s_{t+1})$---as well as $a_{t+1}$ for SARSA---the values are updated with
  \begin{align}
    Q_t(s_t, a_t) &+=  \alpha_t [r_t + \gamma \max_a Q_t(s_{t+1}, a) -  Q_t(s_t, a_t)] \tag{Q-Learning}\\
    Q_t(s_t, a_t) &+=  \alpha_t [r_t + \gamma Q_t(s_{t+1}, a_{t+1}) -  Q_t(s_t, a_t)] \tag{SARSA}.
  \end{align}
  Implement a version of these algorithms where, after every step $t$ taken in the real environment, you perform one or more updates over (some, or all) previously collected data $s_1, a_1, r_1, \ldots, s_k, a_k, r_k, \ldots, s_t, a_t, r_t$.
  \begin{align}
    Q_n(s_k, a_k) &+=  \alpha_n [r_k + \gamma \max_a Q_n(s_{k+1}, a) -  Q_n(s_k, a_k)] \tag{Q-Learning}\\
    Q_n(s_k, a_k) &+=  \alpha_n [r_k + \gamma Q_n(s_{k+1}, a_{k+1}) -  Q_n(s_k, a_k)] \tag{SARSA}.
  \end{align}
  Keeping in mind that Q-Learning is an off-policy algorithm (i.e. it estimates the value of the optimal policy while using data from any arbitrary policy), while SARSA is an on-policy algorithm (i.e. it estimates the value of the policy used to collect the data), which of the two methods would you expect to perform better after this modification? Is this borne out by your experiments?
\end{itemize}

\end{document}
