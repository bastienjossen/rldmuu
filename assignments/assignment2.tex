\documentclass[twoside,a4paper]{article}
% \usepackage{enumerate}
% \setbeamertemplate{enumerate item}{(\alph{enumi})}
% \setbeamertemplate{enumerate subitem}{(\roman{enumii})}
\input{preamble}


\pagestyle{myheadings}



\begin{document}
\title{Exercise set 2}

\author{Christos Dimitrakakis: \texttt{christos.dimitrakakis@unine.ch}}

\maketitle
\large{Deadline: 5 March 2024}


\vspace{1em}


\begin{exercise}
  Complete the meteorologists exercise for the Bayesian and Maximum-A-Posteriori models.
\end{exercise}

\begin{exercise}
  Consider a  Bernoulli random variable with unknown mean
  \[
    x_t \mid \mu \sim \Bernoulli(\mu),
  \]
  where the mean is sampled from a Beta distribution
  \[
    \mu \sim \Beta(\alpha, \beta).
  \]
  The Bernoulli distribution satisfies
  \[
    P_\mu(x_1, \ldots, x_T) = \prod_{t=1}^T P_\mu(x_t) = \prod_{t=1}^T \mu^{s_T} (1 - \mu)^{T - s_T},
  \]
  where $s_T = \sum_{t=1}^T x_T$, while the Beta distribution has the form
  \[
    \bel(\mu) \defn \frac{1}{B(\alpha, \beta)} \mu^{\alpha - 1} (1 - \mu)^{\beta - 1},
  \]
  where the factor $B(\alpha, \beta)$ is a normalisation constant independent of $\mu$ that ensures the density integrates to one.
  \begin{enumerate}
  \item Show that the maximum likelihood estimate
    \[
      \argmax_\mu P_\mu(x_1, \ldots, x_T) 
    \]
    is $\mu = s_T/T$.
  \item Show that the maximum a posteriori estimate
    \[
      \argmax_\mu P_\mu(x_1, \ldots, x_T) \bel(\mu)
    \]
    is $\mu = (\alpha + s_T - 1) / (\alpha + \beta + T - 2)$.
  \item Show that the posterior expected value
    \[
      \E_\bel(\mu | x_1, \ldots, x_t) = \int_{-\infty}^\infty \mu \, d \bel(\mu \mid x_1, \ldots, x_T)
    \]
    is $\mu = (\alpha + s_T) / (\alpha + \beta + T)$.
  \end{enumerate}
  
\end{exercise}
\if\solution 1
  \begin{proof}[Exercise 2 solution]
    To make this simpler, we can take the logarithm of the likelihood
    \begin{align*}
      \ln P_\mu(x_1, \ldots, x_T)
      &=  s_T \ln \mu + (T - s_T) \ln (1 - \mu)
    \end{align*}
    Since $\ln$ is a monotonic function, finding the maximum of the log-likelihood is equivalent to finding the maximum of the likelihood. We can do so by taking the derivative and setting it to zero:
    \begin{align*}
      0 &= \frac{d}{d\mu} \ln P_\mu(x_1, \ldots, x_T)\\
        &=  \frac{d}{d\mu} s_T \ln \mu + \frac{d}{d\mu}  (T - s_T) \ln (1 - \mu)\\
        &=   s_T/\mu  - (T - s_T) / (1 - \mu)\\
      \mu &=   s_T /T.
    \end{align*}
    For the \emph{maximum a posteriori} estimate, we can use the same approach
    \begin{align*}
      0 &=\frac{d}{d\mu} \ln [P_\mu(x_1, \ldots, x_T) \bel(\mu)]\\
      &=
        \frac{d}{d\mu} \ln P_\mu(x_1, \ldots, x_T) + \frac{d}{d\mu} \ln \bel(\mu)\\
      &=
        \frac{s_T}{\mu} - \frac{T - s_T}{1 - \mu}
        +
        \frac{\alpha - 1}{\mu} -  \frac{\beta - 1}{1 - \mu}\\
      &=
        \frac{s_T + \alpha - 1}{\mu}  - \frac{T - s_T + \beta - 1}{1 - \mu}\\
      (s_T + \alpha - 1)(1 - \mu) &= (T - s_T + \beta - 1) \mu\\
      (s_T + \alpha - 1)  &= (T + \beta + \alpha - 2 ) \mu\\
      \mu & = \frac{s_T + \alpha - 1}{T + \beta + \alpha - 2 }
    \end{align*}
    Finally, we can look at the expected value of the distribution. Here we can use a shortcut. We know that the expected value of a Beta distribution with parameters $\alpha, \beta$ is $\alpha / (\alpha + \beta)$. Since we now have a Beta distribution with parameters $\alpha + s_T, \beta + T - s_T$, the expected value is $\alpha + s_T / (\alpha + \beta + T)$.
  \end{proof}
\fi

\begin{exercise}
  James Randi claims that he is \emph{psychic} and can \emph{always predict a coin toss}. Let $A$ denote the event that JR is psychic and let $P(A) = 2^{-16}$ be your prior belief that JR is a psychic. Let us now make a set of bets about Randi.
  In the following, make the following assumptions
  \begin{assumption}
    All experiments are conducted with a fair coin, whose probability of coming heads at the $k$-th toss is $P(H_k) = 1/2$ and where $H_k$ is independent of $H_{k-1}, \ldots, H_1$.
  \end{assumption}
  \begin{assumption}
    If JR is a psychic, then he can perfectly predict the coin tosses. Thus, if $B_k$ is the event that he predicts correctly, then $P(B_k \mid A, H_k) = P(B_k \mid A) = 1$.
  \end{assumption}


  
  \begin{enumerate}
  \item Let $B_k$ denote the event that JR predicts the $k$-th coin toss correctly. What are the dependencies between $A, H_k, B_k$? (You can draw a graphical model to represent them). For simplicity, take $k=1, 2$ only.
  \item What is the marginal probability $P(B_1)$? What is the marginal probability $P(B_2)$?  
  \item Say that JR predicts the first coin toss correctly, i.e. $B_1$ holds. What is then the marginal probability $P(B_2 \mid B_1)$? 
  \end{enumerate}
  \emph{Hint: The only important events for the calculations are whether JR predicts correctly or not.}
\end{exercise}


\begin{exercise}
  We continue the previous exercise, but with the following extra assumption.
  \begin{assumption}
    Your utility for money is linear, i.e. $U(x) = x$ for
    any amount of money $x$.
  \end{assumption}     
  \begin{enumerate}  
  \item At the beginning of the experiment, JR bets you 100 CU that he can predict the \emph{next four} coin tosses, i.e. he is willing to give you 100 CU if he doesn't predict all four tosses. How much are you willing to bet against that, i.e. how much are you willing to pay if he does predict them correctly?
  \item You throw the coin 4 times, and JR guesses correctly all four
    times. Randi now bets you another 100 CU that he can predict the
    \emph{next} four coin tosses. Up to how much would you bet now?
  \end{enumerate}
\end{exercise}


\begin{exercise}[DO NOT HAND IN]
  Now assume that JR is a psychic, but that he can only predict some proportion $\theta \in [0,1]$ of tosses (he could also be a \emph{bad} psychic).
We shall model our uncertainty about $\theta$ with a Beta distribution $\bel(\theta)$ with parameters $(2, 1)$, $\Beta(2,1)$, which places higher probability on all values of $\theta$ closer to $1$. So now our prior looks like this:
\begin{align}
  P(A) &= 1\\
  P(B \mid A) &= \int_0^1 P(B \mid \theta) \bel(\theta) d\theta = \int_0^1 \theta \bel(\theta) d\theta = \E_\bel \theta.\\
\end{align}
So now we know that JR is psychic, but not how good a psychic he is.
\begin{enumerate}
\item What is the resulting posterior distribution $P(B_2 \mid B_1)$ of JR predicting the next toss correctly after he has predicted one correctly? Write this in terms of the resulting Beta posterior parameters, given that $P(A) = 2^{-16}$.
\item Write an expression for the marginal probability $P(B_{n+1} \mid B^n)$, where $B^n$ is the event that he has predicted correctly $n$ times.
\end{enumerate}

\end{exercise}


\begin{exercise}[DO NOT HAND IN]
  The following is an example of a \emph{hierarchical model.}
  Instead of assuming that Randi is either a perfect psychic or a fraud, we can relax our assumption. Let $A$ denote the event that Randi is psychic, but that he can only predict some proportion $\theta \in [0,1]$ of tosses (he could also be a \emph{bad} psychic).
We shall model our uncertainty about $\theta$ with a Beta distribution $\bel(\theta)$ with parameters $(2, 1)$, $\Beta(2,1)$, which places higher probability on all values of $\theta$ closer to $1$. So now our prior marginal distribution looks like this:
\begin{align}
  P(B) &= P(B \mid A) P(A) + P(B \mid \neg A) P(\neg A)\\
  P(B \mid A) &= \int_0^1 P(B \mid \theta) \bel(\theta) d\theta = \int_0^1 \theta \bel(\theta) d\theta = \E_\bel \theta\\
  P(B \mid \neg A) &= \frac{1}{2}.
\end{align}
So now we think that JR is either a psychic or not, but if he is a psychic, we don't know how well he can predict. Still, we place a special emphasis on the case that he is not a psychic, where we'd expect him to predict correctly 50\% of the time.
\begin{enumerate}
\item What is the resulting posterior distribution $P(B_2 \mid B_1)$ of JR predicting the next toss correctly after he has predicted one correctly? Write this in terms of the resulting $P(A \mid B_1)$ and the resulting Beta posterior parameters, given that $P(A) = 2^{-16}$.
\item Write an expression for the marginal probability $P(B_{n+1} \mid B^n)$.
\item Randi now offers you the opportunity to bet 100CU at every time step $t$ that he would guess the next coin toss, with equal odds (i.e. you get 100CU if he doesn't guess correctly and lose 100CU if he does). If you accept the bet at time $t$, you throw the coin and Randi guesses, and you get/pay money depending on the outcome. Then he offers another bet at time $t+1$. If at any time you decline the bet, the game stops. Define a strategy for playing this game.
\end{enumerate}

\end{exercise}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
