\documentclass[twoside,a4paper]{article}
\usepackage[notheorems]{beamerarticle}
\usepackage{listings}
\usepackage{color}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{psfrag}
\usepackage{url}
\usepackage{listings}
 \usepackage{enumerate}
\usepackage{xr}
\externaldocument{book}

% \setbeamertemplate{enumerate item}{(\alph{enumi})}
% \setbeamertemplate{enumerate subitem}{(\roman{enumii})}
\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{ %
  language=Octave,                % the language of the code
  basicstyle=\small,           % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=2,                   % the step between two line-numbers. If it's 1, each line 
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},      % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                   % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},          % keyword style
  commentstyle=\color{dkgreen},       % comment style
  stringstyle=\color{mauve},         % string literal style
  escapeinside={\%*}{*},            % if you want to add LaTeX within your code
  morekeywords={*,...},              % if you want to add more keywords to the set
  deletekeywords={...}              % if you want to delete keywords from the given language
}

\input{preamble}


\pagestyle{myheadings}
\markboth{Assignment 4}{Decision Making under Uncertainty}

%\newcommand\sinc{\mathop{\rm sinc}}

\def\solution {1}

\begin{document}

\Large{\bf Assignment 5. Infinite vs Finite Horizon}
\vspace{1em}
\\
\large{Christos Dimitrakakis:} \texttt{christos.dimitrakakis@unine.ch}

\textbf{Time and scoring} The numbers indicate expected time to complete the exercise. If you take more time than indicated, please note that. A ``!'' indicates that it may require some extra thought. A ``?'' indicates that this is an open question. The exercises count towards your grade.

These particular exercises are mainly programming exercises. They are not trivial, but they should serve to give you some insight about the behaviour of the core MDP algorithms. These algorithms are fundamental in understanding the more advanced approximate algorithms used in practical problems.

\section{A very simple Markov decision process}
Consider the following simple Markov decision process:
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[scale=4, >=triangle 45]
    \node[place] at (0, 0) (s1) {$s_1$};
    \node[place] at (1, 0) (s2)  {$s_2$};
    \draw[->] (s1) to [bend left] node [above] {$0.5$} (s2);
    \draw[->] (s1) to [loop above] node [above] {$0.5$} (s1);
    \draw[->] (s2) to [loop above] node [above] {$1$} (s2);
    \draw[->, dashed] (s1) edge [bend right] node [below] {$1$} (s2);
    \draw[->, dashed] (s2) to [loop below] node [below] {$1$} (s2);
  \end{tikzpicture}
  \caption{A two-state MDP. There are two actions, and the numbers indicate the transition probabilities.}
  \label{fig:two-state-mdp}
\end{figure}
There are two actions $a_1$, indicated by a solid line, and $a_2$, indicated by a dashed line. The diagram shows the transition probabilities. 

The rewards for the two actions are:
\begin{align}
  \label{eq:1}
  r(s_1, a_1) &= 5, & r(s_2, a_1) &= -1,
  \\
  r(s_1, a_2) &= 10, & r(s_2, a_2) &= -1.
\end{align}
So, in this problem $s_2$ is a bad state. However, the optimal policy depends critically on the horizon and the discount factor. If the horizon is short, or the discount factor is small, it should be better to take action $a_2$, which gives a reward of $10$.

\section{The simple MDP as a test-bench}
\label{sec:simple-mdp-as}
\begin{exercise}[30-60!]
  Calculate (can be done analytically) the values of different policies for different horizons, with no discounting, i.e. $\disc = 1$.
  \begin{enumerate}
  \item Calculate the value of the policy always taking action $a_1$ for a finite horizon $T$. 
  \item Calculate the value of the policy always taking action $a_2$ for a finite horizon $T$. 
  \item When does one policy become better than the other?
  \end{enumerate}
  \label{exercise:simple-mdp-finite-horizon}
\end{exercise}


\begin{exercise}[30-60!]
  Repeat the above exercise for $T \to \infty$, but with a varying discount factor $\gamma \in [0,1)$.  (This can also be done analytially!)
  \begin{enumerate}
  \item Calculate the value of the policy always taking action $a_1$ for varying $\gamma$
  \item Calculate the value of the policy always taking action $a_2$ for varying $\gamma$. 
  \item When does one policy become better than the other?
  \end{enumerate}
  \label{exercise:simple-mdp-discounted}
\end{exercise}


\begin{exercise}[60-120]
  Implement the above MDP in \verb|src/RL/simple_MDP.py|
  
  Now implement and verify a number of algorithms. I suggest the following sequence.
  \begin{enumerate}
  \item Use your implementation of backwards induction for finite $T$. Do your analytical results hold until the change point? If not, why not?
  \item Modify backwards induction to obtain the value iteration algorithm for $T \to \infty$, and discounting $\disc \in [0, 1)$. You should obtain a clear change point.
  \item Modify the value iteration algorithm to accept a fixed policy as an input, i.e. so that it does not perform maximisation. Then you end up with a policy evaluation algorithm using dynamic programming. Verify that the two heuristic policies of always taking action $a_1$ and $a_2$ respectively have the values that you have calculated.
  \item Calculate the transition kernel of the MDP and verify that the matrix-inversion policy evaluation results in the same result as the dynamic programming policy evaluation.
  \item Optionally, implement Monte-Carlo policy evaluation.
  %\item Implement policy iteration using one or more algorithms for policy evaluation. Which one converges faster?
  \end{enumerate}
  \label{exercise:simple-mdp-implementation}
\end{exercise}

\section{Further MDPs}

As a further test, repeat the above exercises for other simple MDPs.

\section{Feedback}


\begin{exercise}
  Finally, some questions about the units covering Markov decision processes,  approximate algorithms and reinforcement learning.
  \begin{enumerate}
  \item Did you find the material interesting?
  \item Did you find it potentially useful?
  \item How much did you already know?
  \item How much had you already seen but did not remember in detail?
  \item How much have you seen for the first time?
  \item Which aspect did you like the most?
  \item Which aspect did you like the least?
  \item Did the exercises help you in understanding the material?
  \item Were the exercises necessary to understand the material?
  \item Feel free to add any further comments.
  \end{enumerate}
\end{exercise}
>>>>>>> f6fb30faa20c6a358560761bd6442d0ef3a809d9

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
