
|-----------------------------------------------+-----------------|
| Prospect Theory                               | Meisam Asgari   |
| Finite MAB                                    | Antoine Demont  |
| Curiosity-driven exploration                  | Jiahui Yu       |
| Convergence of Stochastic Dynamic Programming | Boris Mottet    |
| Soft Actor-Critic                             | Vincent Carrell |
| Information-Directed Reward Learning          | Jonas Fontana   |
| Gran Turismo                                  | Jakub Tiuczek   |
| Point-baesd POMDP                             | Meisam Asgari   |
| Robust Predictable Control                    | Antoine Demont  |
| Alpha-Go Zero                                 | Jonas Fontana   |
| Tree-Based RL                                 | Boris Mottet    |
| Natural Policy Gradient                       | Vincent Carrel  |
| Algorithms for IRL                            | Jiahui Yu       |
| Explainability in DRL                         | Jakub Tiuczek   |
|-----------------------------------------------+-----------------|


* Q-Learning convergence

If we define
\[
\Delta_t(s,u) = Q_t(s,u) - Q^*(s,u),
\]
with
\[
F_t(s_t,u_t) = r + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t,u_t)
\]
Then the Q-Learning update can be written as
\[
Q_{t+1}(s_t,a_t) = (1 - \alpha) Q_t(s_t, a_t) + \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a))
\]
and 
\begin{align*}
Q_{t+1}(s_t,a_t) - Q^*(s_t,a_t)
& = (1 - \alpha) [Q_t(s_t, a_t) - Q^*(s_t, a_t)
+ \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t, a_t))
\\
\Delta_{t+1}(s_t, a_t) 
& =
(1 - \alpha) \Delta_t(s_t, a_t)+ \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t, a_t))
\end{align*}

I now write $E_t[x]$ in place of $E[x | P_t]$ for simplicity, and:
\begin{align*}
|E_t[F_t(s_t,a_t)] 
& = |r + \gamma \sum_j \Pr(j | s_t, a_t) \max_a Q_t(j, a) - E_t[Q^*(s_t,a_t)]|
\\
& = \gamma |\sum_j \Pr(j | s_t, a_t) [\max_a Q_t(j, a) - V^*(j)]|
\\
& = \gamma |\sum_j \Pr(j | s_t, a_t) [\max_a Q_t(j, a) - \max_b Q^*(j, b)]|
\\
& \leq \gamma |\sum_j \Pr(j | s_t, a_t) \max_a |Q_t(j, a) - Q^*(j, a)|
\end{align*}
The last inequality follows from this fact, for two functions $f,g$ defined on the same domain.
$|\max_x f(x) - \max_y g(y)| \leq \max_x |f(x) - g(y)|$.

Proof: consider $\max_x f(x) \geq \max_y g(y)$. Then
$|\max_x f(x) - \max_y g(y) | = \max_x f(x) - \max_y g(y) \leq \max_x f(x) - g(x) \leq \max_x |f(x) - g(x)|$,
Now consider $\max_x f(x) < \max_y g(y)$. Then
$|\max_x f(x) - \max_y g(y) | = \max_y g(y)  - \max_x f(x) \leq \max_y g(y) - f(y) \leq \max_y |g(y) - f(y)| = \max_x |f(x) - g(x)|$

The contraction argument is not given explicitly afterwards, but it not hard either:
It is given here, together with the variance proof:
http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf

