
|------+-----------------------------------------------+-----------------|
|      | Prospect Theory                               | Meisam Asgari   |
|      | Finite MAB                                    | Antoine Demont  |
|      | Curiosity-driven exploration                  | Jiahui Yu       |
|      | Convergence of Stochastic Dynamic Programming | Boris Mottet    |
|      | Soft Actor-Critic                             | Vincent Carrell |
| 4.12 | Information-Directed Reward Learning          | Jonas Fontana   |
| 4.26 | Gran Turismo                                  | Jakub Tiuczek   |
| 4.26 | Point-baesd POMDP                             | Meisam Asgari   |
| 5.10 | Robust Predictable Control                    | Antoine Demont  |
| 5.10 | Alpha-Go Zero                                 | Jonas Fontana   |
| 5.17 | Tree-Based RL                                 | Boris Mottet    |
| 5.17 | Natural Policy Gradient                       | Vincent Carrel  |
| 5.24 | Algorithms for IRL                            | Jiahui Yu       |
| 5.24 | Explainability in DRL                         | Jakub Tiuczek   |
|------+-----------------------------------------------+-----------------|


* Q-Learning convergence

If we define
\[
\Delta_t(s,u) = Q_t(s,u) - Q^*(s,u),
\]
with
\[
F_t(s_t,u_t) = r + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t,u_t)
\]
Then the Q-Learning update can be written as
\[
Q_{t+1}(s_t,a_t) = (1 - \alpha) Q_t(s_t, a_t) + \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a))
\]
and 
\begin{align*}
Q_{t+1}(s_t,a_t) - Q^*(s_t,a_t)
& = (1 - \alpha) [Q_t(s_t, a_t) - Q^*(s_t, a_t)
+ \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t, a_t))
\\
\Delta_{t+1}(s_t, a_t) 
& =
(1 - \alpha) \Delta_t(s_t, a_t)+ \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t, a_t))
\end{align*}

I now write $E_t[x]$ in place of $E[x | P_t]$ for simplicity, and:
\begin{align*}
|E_t[F_t(s_t,a_t)] 
& = |r + \gamma \sum_j \Pr(j | s_t, a_t) \max_a Q_t(j, a) - E_t[Q^*(s_t,a_t)]|
\\
& = \gamma |\sum_j \Pr(j | s_t, a_t) [\max_a Q_t(j, a) - V^*(j)]|
\\
& = \gamma |\sum_j \Pr(j | s_t, a_t) [\max_a Q_t(j, a) - \max_b Q^*(j, b)]|
\\
& \leq \gamma |\sum_j \Pr(j | s_t, a_t) \max_a |Q_t(j, a) - Q^*(j, a)|
\end{align*}
The last inequality follows from this fact, for two functions $f,g$ defined on the same domain.
$|\max_x f(x) - \max_y g(y)| \leq \max_x |f(x) - g(x)|$.

Proof: consider $\max_x f(x) \geq \max_y g(y)$. Then
$|\max_x f(x) - \max_y g(y) | = \max_x f(x) - \max_y g(y) \leq \max_x f(x) - g(x) \leq \max_x |f(x) - g(x)|$,
Now consider $\max_x f(x) < \max_y g(y)$. Then
$|\max_x f(x) - \max_y g(y) | = \max_y g(y)  - \max_x f(x) \leq \max_y g(y) - f(y) \leq \max_y |g(y) - f(y)| = \max_x |f(x) - g(x)|$

The contraction argument is not given explicitly afterwards, but it not hard either:
It is given here, together with the variance proof:
http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf

