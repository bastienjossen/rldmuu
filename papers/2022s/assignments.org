 

|------+-----------------------------------------------+-----------------+-------------------------|
|      | Prospect Theory                               | Meisam Asgari   |                         |
|      | Finite MAB                                    | Antoine Demont  |                         |
|      | Curiosity-driven exploration                  | Jiahui Yu       |                         |
|      | Convergence of Stochastic Dynamic Programming | Boris Mottet    |                         |
|      | Soft Actor-Critic                             | Vincent Carrell |                         |
| 4.12 | Information-Directed Reward Learning          | Jonas Fontana   |                         |
| 4.26 | Gran Turismo                                  | Jakub Tiuczek   | Antoine, Boris          |
| 4.26 | Natural Policy Gradient                       | Vincent Carrel  | Jonas, Jiahui, Meisam   |
| 5.03 | Robust Predictable Control                    | Antoine Demont  | Jakub, Jiahui           |
| 5.03 | Alpha-Go Zero                                 | Jonas Fontana   | Boris, Meisam, Vincent  |
| 5.10 | Tree-Based RL                                 | Boris Mottet    | Antoine, Jakub          |
| 5.10 | Algorithms for IRL                            | Jiahui Yu       | +Vincent, Meisam, Jonas |
| 5.17 | HSVI                                          | Meisam Asgari   | Jiahui, Vincent         |
| 5.17 | Explainability in DRL                         | Jakub Tiuczek   | +Jonas, Antoine, Boris  |
|------+-----------------------------------------------+-----------------+-------------------------|

* Q-Learning convergence

If we define
\[
\Delta_t(s,u) = Q_t(s,u) - Q^*(s,u),
\]
with
\[
F_t(s_t,u_t) = r + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t,u_t)
\]
Then the Q-Learning update can be written as
\[
Q_{t+1}(s_t,a_t) = (1 - \alpha) Q_t(s_t, a_t) + \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a))
\]
and 
\begin{align*}
Q_{t+1}(s_t,a_t) - Q^*(s_t,a_t)
& = (1 - \alpha) [Q_t(s_t, a_t) - Q^*(s_t, a_t)
+ \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t, a_t))
\\
\Delta_{t+1}(s_t, a_t) 
& =
(1 - \alpha) \Delta_t(s_t, a_t)+ \alpha(r_t + \gamma \max_a Q_t(s_{t+1}, a) - Q^*(s_t, a_t))
\end{align*}

I now write $E_t[x]$ in place of $E[x | P_t]$ for simplicity, and:
\begin{align*}
|E_t[F_t(s_t,a_t)] 
& = |r + \gamma \sum_j \Pr(j | s_t, a_t) \max_a Q_t(j, a) - E_t[Q^*(s_t,a_t)]|
\\
& = \gamma |\sum_j \Pr(j | s_t, a_t) [\max_a Q_t(j, a) - V^*(j)]|
\\
& = \gamma |\sum_j \Pr(j | s_t, a_t) [\max_a Q_t(j, a) - \max_b Q^*(j, b)]|
\\
& \leq \gamma |\sum_j \Pr(j | s_t, a_t) \max_a |Q_t(j, a) - Q^*(j, a)|
\end{align*}
The last inequality follows from this fact, for two functions $f,g$ defined on the same domain.
$|\max_x f(x) - \max_y g(y)| \leq \max_x |f(x) - g(x)|$.

Proof: consider $\max_x f(x) \geq \max_y g(y)$. Then
$|\max_x f(x) - \max_y g(y) | = \max_x f(x) - \max_y g(y) \leq \max_x f(x) - g(x) \leq \max_x |f(x) - g(x)|$,
Now consider $\max_x f(x) < \max_y g(y)$. Then
$|\max_x f(x) - \max_y g(y) | = \max_y g(y)  - \max_x f(x) \leq \max_y g(y) - f(y) \leq \max_y |g(y) - f(y)| = \max_x |f(x) - g(x)|$

The contraction argument is not given explicitly afterwards, but it not hard either:
It is given here, together with the variance proof:
http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf


* Grading scale for presentations

Presentations are expected to be clear, but this is aspect is not graded.

Presenters should aim to:

(a) Give an introduction to the setting and background.
(b) Explain the overall paper idea
(c) Go into the details of some of the more interesting technical aspects of the papers.
(d) Discuss paper limitations, future work or relations to other works in a larger context.

The following grading scale illustrates what I expect:

10. Excellent understanding. Explains technical details. Gives good background. Interesting discussion points beyond what the paper addresses.
9. A good overall understanding. Explains details and gives some background or has some interesting discussion.
8. A good overall understanding. Explains many details or gives some background
7. A good overall understanding. Explains some details.
6. A sufficient understanding, only minor errors. Little depth.
5. Shows a basic understanding of the paper. Only a few errors.
4. Many details are wrong, or insufficiently understood.
3. Only some superficial aspects of the paper were explained, there were some errors.
2. At least a couple of things were undestood, even though there were many errors.
1. Almost nothing was understood. What was explained was wrong.
0. Nothing was presented.
